{
  "query": "OpenCV Hand Gesture Identify",
  "nodes": [
    {
      "id": "2109291",
      "arxiv_id": "2407.10902",
      "title": "Interpreting Hand gestures using Object Detection and Digits\n  Classification",
      "abstract": "Hand gestures have evolved into a natural and intuitive means of engaging\nwith technology. The objective of this research is to develop a robust system\nthat can accurately recognize and classify hand gestures representing numbers.\nThe proposed approach involves collecting a dataset of hand gesture images,\npreprocessing and enhancing the images, extracting relevant features, and\ntraining a machine learning model. The advancement of computer vision\ntechnology and object detection techniques, in conjunction with OpenCV's\ncapability to analyze and comprehend hand gestures, presents a chance to\ntransform the identification of numerical digits and its potential\napplications. The advancement of computer vision technology and object\nidentification technologies, along with OpenCV's capacity to analyze and\ninterpret hand gestures, has the potential to revolutionize human interaction,\nboosting people's access to information, education, and employment\nopportunities. Keywords: Computer Vision, Machine learning, Deep Learning,\nNeural Networks",
      "categories": "cs.CV",
      "update_date": "2024-07-16",
      "doi": null,
      "authors": "Sangeetha K, Balaji VS, Kamalesh P, Anirudh Ganapathy PS",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.779527319697899,
      "community_id": null
    },
    {
      "id": "2235578",
      "arxiv_id": "2501.11992",
      "title": "Survey on Hand Gesture Recognition from Visual Input",
      "abstract": "Hand gesture recognition has become an important research area, driven by the growing demand for human-computer interaction in fields such as sign language recognition, virtual and augmented reality, and robotics. Despite the rapid growth of the field, there are few surveys that comprehensively cover recent research developments, available solutions, and benchmark datasets. This survey addresses this gap by examining the latest advancements in hand gesture and 3D hand pose recognition from various types of camera input data including RGB images, depth images, and videos from monocular or multiview cameras, examining the differing methodological requirements of each approach. Furthermore, an overview of widely used datasets is provided, detailing their main characteristics and application domains. Finally, open challenges such as achieving robust recognition in real-world environments, handling occlusions, ensuring generalization across diverse users, and addressing computational efficiency for real-time applications are highlighted to guide future research directions. By synthesizing the objectives, methodologies, and applications of recent studies, this survey offers valuable insights into current trends, challenges, and opportunities for future research in human hand gesture recognition.",
      "categories": "cs.CV cs.AI",
      "update_date": "2025-09-04",
      "doi": "10.1109/ACCESS.2025.3593428",
      "authors": "Manousos Linardakis, Iraklis Varlamis, Georgios Th. Papadopoulos",
      "comments": "37 pages",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.768407716249961,
      "community_id": "20241021_2475Q"
    },
    {
      "id": "1937177",
      "arxiv_id": "2310.14919",
      "title": "GRLib: An Open-Source Hand Gesture Detection and Recognition Python\n  Library",
      "abstract": "Hand gesture recognition systems provide a natural way for humans to interact\nwith computer systems. Although various algorithms have been designed for this\ntask, a host of external conditions, such as poor lighting or distance from the\ncamera, make it difficult to create an algorithm that performs well across a\nrange of environments. In this work, we present GRLib: an open-source Python\nlibrary able to detect and classify static and dynamic hand gestures. Moreover,\nthe library can be trained on existing data for improved classification\nrobustness. The proposed solution utilizes a feed from an RGB camera. The\nretrieved frames are then subjected to data augmentation and passed on to\nMediaPipe Hands to perform hand landmark detection. The landmarks are then\nclassified into their respective gesture class. The library supports dynamic\nhand gestures through trajectories and keyframe extraction. It was found that\nthe library outperforms another publicly available HGR system - MediaPipe\nSolutions, on three diverse, real-world datasets. The library is available at\nhttps://github.com/mikhail-vlasenko/grlib and can be installed with pip.",
      "categories": "cs.CV",
      "update_date": "2023-10-24",
      "doi": null,
      "authors": "Jan Warchocki, Mikhail Vlasenko, Yke Bauke Eisma",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.766560449209188,
      "community_id": null
    },
    {
      "id": "1407666",
      "arxiv_id": "2101.03923",
      "title": "A novel shape matching descriptor for real-time hand gesture recognition",
      "abstract": "The current state-of-the-art hand gesture recognition methodologies heavily\nrely in the use of machine learning. However there are scenarios that machine\nlearning cannot be applied successfully, for example in situations where data\nis scarce. This is the case when one-to-one matching is required between a\nquery and a dataset of hand gestures where each gesture represents a unique\nclass. In situations where learning algorithms cannot be trained, classic\ncomputer vision techniques such as feature extraction can be used to identify\nsimilarities between objects. Shape is one of the most important features that\ncan be extracted from images, however the most accurate shape matching\nalgorithms tend to be computationally inefficient for real-time applications.\nIn this work we present a novel shape matching methodology for real-time hand\ngesture recognition. Extensive experiments were carried out comparing our\nmethod with other shape matching methods with respect to accuracy and\ncomputational complexity using our own collected hand gesture dataset and a\nmodified version of the MPEG-7 dataset.%that is widely used for comparing 2D\nshape matching algorithms. Our method outperforms the other methods and\nprovides a good combination of accuracy and computational efficiency for\nreal-time applications.",
      "categories": "cs.CV",
      "update_date": "2021-03-12",
      "doi": null,
      "authors": "Michalis Lazarou, Bo Li, Tania Stathaki",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.764504699170029,
      "community_id": null
    },
    {
      "id": "2364034",
      "arxiv_id": "2507.04465",
      "title": "Visual Hand Gesture Recognition with Deep Learning: A Comprehensive Review of Methods, Datasets, Challenges and Future Research Directions",
      "abstract": "The rapid evolution of deep learning (DL) models and the ever-increasing size of available datasets have raised the interest of the research community in the always important field of vision-based hand gesture recognition (VHGR), and delivered a wide range of applications, such as sign language understanding and human-computer interaction using cameras. Despite the large volume of research works in the field, a structured and complete survey on VHGR is still missing, leaving researchers to navigate through hundreds of papers in order to find the right combination of data, model, and approach for each task. The current survey aims to fill this gap by presenting a comprehensive overview of this aspect of computer vision. With a systematic research methodology that identifies the state-of-the-art works and a structured presentation of the various methods, datasets, and evaluation metrics, this review aims to constitute a useful guideline for researchers, helping them to choose the right strategy for delving into a certain VHGR task. Starting with the methodology used for study selection, literature retrieval, and the analytical framing, the survey identifies and organizes key VHGR approaches using a taxonomy-based format in various dimensions such as input modality and application domain. The core of the survey provides an in-depth analysis of state-of-the-art techniques across three primary VHGR tasks: static gesture recognition, isolated dynamic gestures and continuous gesture recognition. For each task, the architectural trends and learning strategies are listed. Additionally, the study reviews commonly used datasets - emphasizing on annotation schemes - and evaluates standard performance metrics. It concludes by identifying major challenges in VHGR, including both general computer vision issues and domain-specific obstacles, and outlines promising directions for future research.",
      "categories": "cs.CV",
      "update_date": "2025-07-08",
      "doi": null,
      "authors": "Konstantinos Foteinos, Jorgen Cani, Manousos Linardakis, Panagiotis Radoglou-Grammatikis, Vasileios Argyriou, Panagiotis Sarigiannidis, Iraklis Varlamis, Georgios Th. Papadopoulos",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.756581022943199,
      "community_id": "20241021_2475Q"
    },
    {
      "id": "1401067",
      "arxiv_id": "2012.13188",
      "title": "Control of Computer Pointer Using Hand Gesture Recognition in Motion\n  Pictures",
      "abstract": "This paper presents a user interface designed to enable computer cursor\ncontrol through hand detection and gesture classification. A comprehensive hand\ndataset comprising 6720 image samples was collected, encompassing four distinct\nclasses: fist, palm, pointing to the left, and pointing to the right. The\nimages were captured from 15 individuals in various settings, including simple\nbackgrounds with different perspectives and lighting conditions. A\nconvolutional neural network (CNN) was trained on this dataset to accurately\npredict labels for each captured image and measure their similarity. The system\nincorporates defined commands for cursor movement, left-click, and right-click\nactions. Experimental results indicate that the proposed algorithm achieves a\nremarkable accuracy of 91.88% and demonstrates its potential applicability\nacross diverse backgrounds.",
      "categories": "cs.CV cs.HC",
      "update_date": "2023-06-13",
      "doi": null,
      "authors": "Yalda Foroutan, Ahmad Kalhor, Saeid Mohammadi Nejati, Samad Sheikhaei",
      "comments": "9 pages, 6 figures, 2 tables",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.756260855866519,
      "community_id": null
    },
    {
      "id": "289403",
      "arxiv_id": "1109.5034",
      "title": "Natural hand gestures for human identification in a Human-Computer\n  Interface",
      "abstract": "The goal of this work is the identification of humans based on motion data in\nthe form of natural hand gestures. In this paper, the identification problem is\nformulated as classification with classes corresponding to persons' identities,\nbased on recorded signals of performed gestures. The identification performance\nis examined with a database of twenty-two natural hand gestures recorded with\ntwo types of hardware and three state-of-art classifiers: Linear Discrimination\nAnalysis (LDA), Support Vector machines (SVM) and k-Nearest Neighbour (k-NN).\nResults show that natural hand gestures allow for an effective human\nclassification.",
      "categories": "cs.HC",
      "update_date": "2013-03-20",
      "doi": null,
      "authors": "Micha{\\l} Romaszewski and Przemys{\\l}aw G{\\l}omb and Piotr Gawron",
      "comments": "13 pages, 3 figures, This is a major rewrite of previous version of\n  the paper. The same dataset as in previous version was used. The analysis is\n  now focused on application of the gestures classification methods to human\n  identification",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.752125173553804,
      "community_id": null
    },
    {
      "id": "1377609",
      "arxiv_id": "2011.04860",
      "title": "Understanding the hand-gestures using Convolutional Neural Networks and\n  Generative Adversial Networks",
      "abstract": "In this paper, it is introduced a hand gesture recognition system to\nrecognize the characters in the real time. The system consists of three\nmodules: real time hand tracking, training gesture and gesture recognition\nusing Convolutional Neural Networks. Camshift algorithm and hand blobs analysis\nfor hand tracking are being used to obtain motion descriptors and hand region.\nIt is fairy robust to background cluster and uses skin color for hand gesture\ntracking and recognition. Furthermore, the techniques have been proposed to\nimprove the performance of the recognition and the accuracy using the\napproaches like selection of the training images and the adaptive threshold\ngesture to remove non-gesture pattern that helps to qualify an input pattern as\na gesture. In the experiments, it has been tested to the vocabulary of 36\ngestures including the alphabets and digits, and results effectiveness of the\napproach.",
      "categories": "cs.CV",
      "update_date": "2020-11-11",
      "doi": null,
      "authors": "Arpita Vats",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.748778015520809,
      "community_id": null
    },
    {
      "id": "1802119",
      "arxiv_id": "2303.02321",
      "title": "Real-Time Hand Gesture Identification in Thermal Images",
      "abstract": "Hand gesture-based human-computer interaction is an important problem that is\nwell explored using color camera data. In this work we proposed a hand gesture\ndetection system using thermal images. Our system is capable of handling\nmultiple hand regions in a frame and process it fast for real-time\napplications. Our system performs a series of steps including background\nsubtraction-based hand mask generation, k-means based hand region\nidentification, hand segmentation to remove the forearm region, and a\nConvolutional Neural Network (CNN) based gesture classification. Our work\nintroduces two novel algorithms, bubble growth and bubble search, for faster\nhand segmentation. We collected a new thermal image data set with 10 gestures\nand reported an end-to-end hand gesture recognition accuracy of 97%.",
      "categories": "cs.CV cs.AI",
      "update_date": "2023-03-07",
      "doi": "10.1007/978-3-031-06430-2_41",
      "authors": "James Ballow, Soumyabrata Dey",
      "comments": "21st International Conference on Image Analysis and Processing",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.745056346353512,
      "community_id": null
    },
    {
      "id": "547066",
      "arxiv_id": "1408.1759",
      "title": "Real-Time and Robust Method for Hand Gesture Recognition System Based on\n  Cross-Correlation Coefficient",
      "abstract": "Hand gesture recognition possesses extensive applications in virtual reality,\nsign language recognition, and computer games. The direct interface of hand\ngestures provides us a new way for communicating with the virtual environment.\nIn this paper a novel and real-time approach for hand gesture recognition\nsystem is presented. In the suggested method, first, the hand gesture is\nextracted from the main image by the image segmentation and morphological\noperation and then is sent to feature extraction stage. In feature extraction\nstage the Cross-correlation coefficient is applied on the gesture to recognize\nit. In the result part, the proposed approach is applied on American Sign\nLanguage (ASL) database and the accuracy rate obtained 98.34%.",
      "categories": "cs.CV",
      "update_date": "2014-08-11",
      "doi": null,
      "authors": "Reza Azad, Babak Azad, Iman Tavakoli Kazerooni",
      "comments": "arXiv admin note: substantial text overlap with\n  http://dx.doi.org/10.1109/ICCCA.2012.6179213 by other author",
      "journal_ref": "Advances in Computer Science: an International Journal, Vol. 2,\n  Issue 5, No.6, pp. 121-125, 2013",
      "citation_count": null,
      "relevance": 0.744913772051707,
      "community_id": null
    },
    {
      "id": "2007304",
      "arxiv_id": "2402.09663",
      "title": "Hand Shape and Gesture Recognition using Multiscale Template Matching,\n  Background Subtraction and Binary Image Analysis",
      "abstract": "This paper presents a hand shape classification approach employing multiscale\ntemplate matching. The integration of background subtraction is utilized to\nderive a binary image of the hand object, enabling the extraction of key\nfeatures such as centroid and bounding box. The methodology, while simple,\ndemonstrates effectiveness in basic hand shape classification tasks, laying the\nfoundation for potential applications in straightforward human-computer\ninteraction scenarios. Experimental results highlight the system's capability\nin controlled environments.",
      "categories": "cs.CV",
      "update_date": "2024-02-16",
      "doi": null,
      "authors": "Ketan Suhaas Saichandran",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.743116952684019,
      "community_id": null
    },
    {
      "id": "1074170",
      "arxiv_id": "1901.04622",
      "title": "Fast and Robust Dynamic Hand Gesture Recognition via Key Frames\n  Extraction and Feature Fusion",
      "abstract": "Gesture recognition is a hot topic in computer vision and pattern\nrecognition, which plays a vitally important role in natural human-computer\ninterface. Although great progress has been made recently, fast and robust hand\ngesture recognition remains an open problem, since the existing methods have\nnot well balanced the performance and the efficiency simultaneously. To bridge\nit, this work combines image entropy and density clustering to exploit the key\nframes from hand gesture video for further feature extraction, which can\nimprove the efficiency of recognition. Moreover, a feature fusion strategy is\nalso proposed to further improve feature representation, which elevates the\nperformance of recognition. To validate our approach in a \"wild\" environment,\nwe also introduce two new datasets called HandGesture and Action3D datasets.\nExperiments consistently demonstrate that our strategy achieves competitive\nresults on Northwestern University, Cambridge, HandGesture and Action3D hand\ngesture datasets. Our code and datasets will release at\nhttps://github.com/Ha0Tang/HandGestureRecognition.",
      "categories": "cs.CV",
      "update_date": "2019-01-16",
      "doi": null,
      "authors": "Hao Tang, Hong Liu, Wei Xiao, Nicu Sebe",
      "comments": "11 pages, 3 figures, accepted to NeuroComputing",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.742019728301781,
      "community_id": null
    },
    {
      "id": "1747974",
      "arxiv_id": "2211.09067",
      "title": "Egocentric Hand-object Interaction Detection",
      "abstract": "In this paper, we propose a method to jointly determine the status of\nhand-object interaction. This is crucial for egocentric human activity\nunderstanding and interaction. From a computer vision perspective, we believe\nthat determining whether a hand is interacting with an object depends on\nwhether there is an interactive hand pose and whether the hand is touching the\nobject. Thus, we extract the hand pose, hand-object masks to jointly determine\nthe interaction status. In order to solve the problem of hand pose estimation\ndue to in-hand object occlusion, we use a multi-cam system to capture hand pose\ndata from multiple perspectives. We evaluate and compare our method with the\nmost recent work from Shan et al. \\cite{Shan20} on selected images from\nEPIC-KITCHENS \\cite{damen2018scaling} dataset and achieve $89\\%$ accuracy on\nHOI (hand-object interaction) detection which is comparative to Shan's\n($92\\%$). However, for real-time performance, our method can run over\n$\\textbf{30}$ FPS which is much more efficient than Shan's\n($\\textbf{1}\\sim\\textbf{2}$ FPS). A demo can be found from\nhttps://www.youtube.com/watch?v=XVj3zBuynmQ",
      "categories": "cs.CV cs.HC",
      "update_date": "2022-11-17",
      "doi": null,
      "authors": "Yao Lu, Yanan Liu",
      "comments": "arXiv admin note: substantial text overlap with arXiv:2109.14734",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.741208166932086,
      "community_id": "20241021_11SBM"
    },
    {
      "id": "2006061",
      "arxiv_id": "2402.08420",
      "title": "Vision-Based Hand Gesture Customization from a Single Demonstration",
      "abstract": "Hand gesture recognition is becoming a more prevalent mode of human-computer\ninteraction, especially as cameras proliferate across everyday devices. Despite\ncontinued progress in this field, gesture customization is often underexplored.\nCustomization is crucial since it enables users to define and demonstrate\ngestures that are more natural, memorable, and accessible. However,\ncustomization requires efficient usage of user-provided data. We introduce a\nmethod that enables users to easily design bespoke gestures with a monocular\ncamera from one demonstration. We employ transformers and meta-learning\ntechniques to address few-shot learning challenges. Unlike prior work, our\nmethod supports any combination of one-handed, two-handed, static, and dynamic\ngestures, including different viewpoints, and the ability to handle irrelevant\nhand movements. We implement three real-world applications using our\ncustomization method, conduct a user study, and achieve up to 94% average\nrecognition accuracy from one demonstration. Our work provides a viable path\nfor vision-based gesture customization, laying the foundation for future\nadvancements in this domain.",
      "categories": "cs.HC",
      "update_date": "2024-10-04",
      "doi": "10.1145/3654777.3676378",
      "authors": "Soroush Shahi, Vimal Mollyn, Cori Tymoszek Park, Richard Kang, Asaf\n  Liberman, Oron Levy, Jun Gong, Abdelkareem Bedri, Gierad Laput",
      "comments": "2024 (UIST' 24). USA, 14 pages",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.740296304225927,
      "community_id": null
    },
    {
      "id": "1554142",
      "arxiv_id": "2111.00038",
      "title": "On-device Real-time Hand Gesture Recognition",
      "abstract": "We present an on-device real-time hand gesture recognition (HGR) system,\nwhich detects a set of predefined static gestures from a single RGB camera. The\nsystem consists of two parts: a hand skeleton tracker and a gesture classifier.\nWe use MediaPipe Hands as the basis of the hand skeleton tracker, improve the\nkeypoint accuracy, and add the estimation of 3D keypoints in a world metric\nspace. We create two different gesture classifiers, one based on heuristics and\nthe other using neural networks (NN).",
      "categories": "cs.CV",
      "update_date": "2021-11-02",
      "doi": null,
      "authors": "George Sung, Kanstantsin Sokal, Esha Uboweja, Valentin Bazarevsky,\n  Jonathan Baccash, Eduard Gabriel Bazavan, Chuo-Ling Chang, Matthias Grundmann",
      "comments": "5 pages, 6 figures; ICCV Workshop on Computer Vision for Augmented\n  and Virtual Reality, Montreal, Canada, 2021",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.739105917838043,
      "community_id": "20241021_1FDMU"
    },
    {
      "id": "1056631",
      "arxiv_id": "1811.11997",
      "title": "Hand Gesture Detection and Conversion to Speech and Text",
      "abstract": "The hand gestures are one of the typical methods used in sign language. It is\nvery difficult for the hearing-impaired people to communicate with the world.\nThis project presents a solution that will not only automatically recognize the\nhand gestures but will also convert it into speech and text output so that\nimpaired person can easily communicate with normal people. A camera attached to\ncomputer will capture images of hand and the contour feature extraction is used\nto recognize the hand gestures of the person. Based on the recognized gestures,\nthe recorded soundtrack will be played.",
      "categories": "cs.CV",
      "update_date": "2018-11-30",
      "doi": null,
      "authors": "K. Manikandan, Ayush Patidar, Pallav Walia, Aneek Barman Roy",
      "comments": "5 pages, 5 figures, International Conference on Innovations and\n  Discoveries in Science, Engineering and Technology(ICIDSET) 2018",
      "journal_ref": "International Journal of Pure and Applied Mathematics, Volume 120\n  No. 6 2018, 1347-1362, ISSN: 1314-3395 (on-line version)",
      "citation_count": null,
      "relevance": 0.73685808558118,
      "community_id": null
    },
    {
      "id": "2227588",
      "arxiv_id": "2501.04002",
      "title": "Extraction Of Cumulative Blobs From Dynamic Gestures",
      "abstract": "Gesture recognition is a perceptual user interface, which is based on CV\ntechnology that allows the computer to interpret human motions as commands,\nallowing users to communicate with a computer without the use of hands, thus\nmaking the mouse and keyboard superfluous. Gesture recognition's main weakness\nis a light condition because gesture control is based on computer vision, which\nheavily relies on cameras. These cameras are used to interpret gestures in 2D\nand 3D, so the extracted information can vary depending on the source of light.\nThe limitation of the system cannot work in a dark environment. A simple night\nvision camera can be used as our camera for motion capture as they also blast\nout infrared light which is not visible to humans but can be clearly seen with\na camera that has no infrared filter this majorly overcomes the limitation of\nsystems which cannot work in a dark environment. So, the video stream from the\ncamera is fed into a Raspberry Pi which has a Python program running OpenCV\nmodule which is used for detecting, isolating and tracking the path of dynamic\ngesture, then we use an algorithm of machine learning to recognize the pattern\ndrawn and accordingly control the GPIOs of the raspberry pi to perform some\nactivities.",
      "categories": "cs.CV",
      "update_date": "2025-01-08",
      "doi": null,
      "authors": "Rishabh Naulakha, Shubham Gaur, Dhairya Lodha, Mehek Tulsyan, Utsav\n  Kotecha",
      "comments": null,
      "journal_ref": "International Journal of Scientific Research, Volume 10, Issue 9,\n  September 2021, pp. 4-7",
      "citation_count": null,
      "relevance": 0.735252634569847,
      "community_id": null
    },
    {
      "id": "1537556",
      "arxiv_id": "2109.14734",
      "title": "Egocentric Hand-object Interaction Detection and Application",
      "abstract": "In this paper, we present a method to detect the hand-object interaction from\nan egocentric perspective. In contrast to massive data-driven discriminator\nbased method like \\cite{Shan20}, we propose a novel workflow that utilises the\ncues of hand and object. Specifically, we train networks predicting hand pose,\nhand mask and in-hand object mask to jointly predict the hand-object\ninteraction status. We compare our method with the most recent work from Shan\net al. \\cite{Shan20} on selected images from EPIC-KITCHENS\n\\cite{damen2018scaling} dataset and achieve $89\\%$ accuracy on HOI (hand-object\ninteraction) detection which is comparative to Shan's ($92\\%$). However, for\nreal-time performance, with the same machine, our method can run over\n$\\textbf{30}$ FPS which is much efficient than Shan's\n($\\textbf{1}\\sim\\textbf{2}$ FPS). Furthermore, with our approach, we are able\nto segment script-less activities from where we extract the frames with the HOI\nstatus detection. We achieve $\\textbf{68.2\\%}$ and $\\textbf{82.8\\%}$ F1 score\non GTEA \\cite{fathi2011learning} and the UTGrasp \\cite{cai2015scalable} dataset\nrespectively which are all comparative to the SOTA methods.",
      "categories": "cs.CV",
      "update_date": "2021-10-01",
      "doi": null,
      "authors": "Yao Lu, Walterio W. Mayol-Cuevas",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.734244855136293,
      "community_id": "20241021_11SBM"
    },
    {
      "id": "1079871",
      "arxiv_id": "1901.10323",
      "title": "Real-time Hand Gesture Detection and Classification Using Convolutional\n  Neural Networks",
      "abstract": "Real-time recognition of dynamic hand gestures from video streams is a\nchallenging task since (i) there is no indication when a gesture starts and\nends in the video, (ii) performed gestures should only be recognized once, and\n(iii) the entire architecture should be designed considering the memory and\npower budget. In this work, we address these challenges by proposing a\nhierarchical structure enabling offline-working convolutional neural network\n(CNN) architectures to operate online efficiently by using sliding window\napproach. The proposed architecture consists of two models: (1) A detector\nwhich is a lightweight CNN architecture to detect gestures and (2) a classifier\nwhich is a deep CNN to classify the detected gestures. In order to evaluate the\nsingle-time activations of the detected gestures, we propose to use Levenshtein\ndistance as an evaluation metric since it can measure misclassifications,\nmultiple detections, and missing detections at the same time. We evaluate our\narchitecture on two publicly available datasets - EgoGesture and NVIDIA Dynamic\nHand Gesture Datasets - which require temporal detection and classification of\nthe performed hand gestures. ResNeXt-101 model, which is used as a classifier,\nachieves the state-of-the-art offline classification accuracy of 94.04% and\n83.82% for depth modality on EgoGesture and NVIDIA benchmarks, respectively. In\nreal-time detection and classification, we obtain considerable early detections\nwhile achieving performances close to offline operation. The codes and\npretrained models used in this work are publicly available.",
      "categories": "cs.CV cs.AI",
      "update_date": "2019-10-21",
      "doi": null,
      "authors": "Okan K\\\"op\\\"ukl\\\"u, Ahmet Gunduz, Neslihan Kose, Gerhard Rigoll",
      "comments": "Published at IEEE International Conference on Automatic Face and\n  Gesture Recognition (FG 2019) - Best student paper award! -",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.732842954058295,
      "community_id": null
    },
    {
      "id": "642905",
      "arxiv_id": "1507.05243",
      "title": "Hand Gesture Recognition Library",
      "abstract": "In this paper we have presented a hand gesture recognition library. Various\nfunctions include detecting cluster count, cluster orientation, finger pointing\ndirection, etc. To use these functions first the input image needs to be\nprocessed into a logical array for which a function has been developed. The\nlibrary has been developed keeping flexibility in mind and thus provides\napplication developers a wide range of options to develop custom gestures.",
      "categories": "cs.CV",
      "update_date": "2015-07-21",
      "doi": null,
      "authors": "Jonathan Fidelis Paul, Dibyabiva Seth, Cijo Paul, Jayati Ghosh\n  Dastidar",
      "comments": null,
      "journal_ref": "International Journal of Science and Applied Information\n  Technology, Volume 3, No.2, March - April 2014",
      "citation_count": null,
      "relevance": 0.73274240694218,
      "community_id": null
    },
    {
      "id": "960478",
      "arxiv_id": "1803.10344",
      "title": "Hand Gesture Controlled Drones: An Open Source Library",
      "abstract": "Drones are conventionally controlled using joysticks, remote controllers,\nmobile applications, and embedded computers. A few significant issues with\nthese approaches are that drone control is limited by the range of\nelectromagnetic radiation and susceptible to interference noise. In this study\nwe propose the use of hand gestures as a method to control drones. We\ninvestigate the use of computer vision methods to develop an intuitive way of\nagent-less communication between a drone and its operator. Computer\nvision-based methods rely on the ability of a drone's camera to capture\nsurrounding images and use pattern recognition to translate images to\nmeaningful and/or actionable information. The proposed framework involves a few\nkey parts toward an ultimate action to be taken. They are: image segregation\nfrom the video streams of front camera, creating a robust and reliable image\nrecognition based on segregated images, and finally conversion of classified\ngestures into actionable drone movement, such as takeoff, landing, hovering and\nso forth. A set of five gestures are studied in this work. Haar feature-based\nAdaBoost classifier is employed for gesture recognition. We also envisage\nsafety of the operator and drone's action calculating the distance based on\ncomputer vision for this task. A series of experiments are conducted to measure\ngesture recognition accuracies considering the major scene variabilities,\nillumination, background, and distance. Classification accuracies show that\nwell-lit, clear background, and within 3 ft gestures are recognized correctly\nover 90%. Limitations of current framework and feasible solutions for better\ngesture recognition are discussed, too. The software library we developed, and\nhand gesture data sets are open-sourced at project website.",
      "categories": "cs.RO",
      "update_date": "2018-03-29",
      "doi": null,
      "authors": "Kathiravan Natarajan, Truong-Huy D. Nguyen, Mutlu Mete",
      "comments": "ICDIS 2018",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.732614727900657,
      "community_id": null
    },
    {
      "id": "1083721",
      "arxiv_id": "1902.02636",
      "title": "Commodifying Pointing in HRI: Simple and Fast Pointing Gesture Detection\n  from RGB-D Images",
      "abstract": "We present and characterize a simple method for detecting pointing gestures\nsuitable for human-robot interaction applications using a commodity RGB-D\ncamera. We exploit a state-of-the-art Deep CNN-based detector to find hands and\nfaces in RGB images, then examine the corresponding depth channel pixels to\nobtain full 3D pointing vectors. We test several methods of estimating the hand\nend-point of the pointing vector. The system runs at better than 30Hz on\ncommodity hardware: exceeding the frame rate of typical RGB-D sensors. An\nestimate of the absolute pointing accuracy is found empirically by comparison\nwith ground-truth data from a VICON motion-capture system, and the useful\ninteraction volume established. Finally, we show an end-to-end test where a\nrobot estimates where the pointing vector intersects the ground plane, and\nreport the accuracy obtained. We provide source code as a ROS node, with the\nintention of contributing a commodity implementation of this common component\nin HRI systems.",
      "categories": "cs.RO",
      "update_date": "2019-02-08",
      "doi": null,
      "authors": "Bita Azari, Angelica Lim and Richard T. Vaughan",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.732314022503612,
      "community_id": null
    },
    {
      "id": "745155",
      "arxiv_id": "1606.07247",
      "title": "Human Computer Interaction Using Marker Based Hand Gesture Recognition",
      "abstract": "Human Computer Interaction (HCI) has been redefined in this era. People want\nto interact with their devices in such a way that has physical significance in\nthe real world, in other words, they want ergonomic input devices. In this\npaper, we propose a new method of interaction with computing devices having a\nconsumer grade camera, that uses two colored markers (red and green) worn on\ntips of the fingers to generate desired hand gestures, and for marker detection\nand tracking we used template matching with kalman filter. We have implemented\nall the usual system commands, i.e., cursor movement, right click, left click,\ndouble click, going forward and backward, zoom in and out through different\nhand gestures. Our system can easily recognize these gestures and give\ncorresponding system commands. Our system is suitable for both desktop devices\nand devices where touch screen is not feasible like large screens or projected\nscreens.",
      "categories": "cs.HC cs.CV",
      "update_date": "2016-06-24",
      "doi": null,
      "authors": "Sayem Mohammad Siam, Jahidul Adnan Sakel, and Md. Hasanul Kabir",
      "comments": "8 Pages, didn't submit to any conference yet",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.732002171039323,
      "community_id": null
    },
    {
      "id": "841809",
      "arxiv_id": "1704.07296",
      "title": "A Real-time Hand Gesture Recognition and Human-Computer Interaction\n  System",
      "abstract": "In this project, we design a real-time human-computer interaction system\nbased on hand gesture. The whole system consists of three components: hand\ndetection, gesture recognition and human-computer interaction (HCI) based on\nrecognition; and realizes the robust control of mouse and keyboard events with\na higher accuracy of gesture recognition. Specifically, we use the\nconvolutional neural network (CNN) to recognize gestures and makes it\nattainable to identify relatively complex gestures using only one cheap\nmonocular camera. We introduce the Kalman filter to estimate the hand position\nbased on which the mouse cursor control is realized in a stable and smooth way.\nDuring the HCI stage, we develop a simple strategy to avoid the false\nrecognition caused by noises - mostly transient, false gestures, and thus to\nimprove the reliability of interaction. The developed system is highly\nextendable and can be used in human-robotic or other human-machine interaction\nscenarios with more complex command formats rather than just mouse and keyboard\nevents.",
      "categories": "cs.CV",
      "update_date": "2017-04-25",
      "doi": null,
      "authors": "Pei Xu",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.731596100640996,
      "community_id": "20241021_PM2H"
    },
    {
      "id": "1381649",
      "arxiv_id": "2011.08900",
      "title": "Whose hand is this? Person Identification from Egocentric Hand Gestures",
      "abstract": "Recognizing people by faces and other biometrics has been extensively studied\nin computer vision. But these techniques do not work for identifying the wearer\nof an egocentric (first-person) camera because that person rarely (if ever)\nappears in their own first-person view. But while one's own face is not\nfrequently visible, their hands are: in fact, hands are among the most common\nobjects in one's own field of view. It is thus natural to ask whether the\nappearance and motion patterns of people's hands are distinctive enough to\nrecognize them. In this paper, we systematically study the possibility of\nEgocentric Hand Identification (EHI) with unconstrained egocentric hand\ngestures. We explore several different visual cues, including color, shape,\nskin texture, and depth maps to identify users' hands. Extensive ablation\nexperiments are conducted to analyze the properties of hands that are most\ndistinctive. Finally, we show that EHI can improve generalization of other\ntasks, such as gesture recognition, by training adversarially to encourage\nthese models to ignore differences between users.",
      "categories": "cs.CV",
      "update_date": "2020-11-19",
      "doi": null,
      "authors": "Satoshi Tsutsui, Yanwei Fu, David Crandall",
      "comments": "Accepted to IEEE Winter Conference on Applications of Computer Vision\n  (WACV) 2021 (First round acceptance)",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.73109252863571,
      "community_id": null
    },
    {
      "id": "485544",
      "arxiv_id": "1312.4190",
      "title": "One-Shot-Learning Gesture Recognition using HOG-HOF Features",
      "abstract": "The purpose of this paper is to describe one-shot-learning gesture\nrecognition systems developed on the \\textit{ChaLearn Gesture Dataset}. We use\nRGB and depth images and combine appearance (Histograms of Oriented Gradients)\nand motion descriptors (Histogram of Optical Flow) for parallel temporal\nsegmentation and recognition. The Quadratic-Chi distance family is used to\nmeasure differences between histograms to capture cross-bin relationships. We\nalso propose a new algorithm for trimming videos --- to remove all the\nunimportant frames from videos. We present two methods that use combination of\nHOG-HOF descriptors together with variants of Dynamic Time Warping technique.\nBoth methods outperform other published methods and help narrow down the gap\nbetween human performance and algorithms on this task. The code has been made\npublicly available in the MLOSS repository.",
      "categories": "cs.CV",
      "update_date": "2014-02-18",
      "doi": null,
      "authors": "Jakub Kone\\v{c}n\\'y and Michal Hagara",
      "comments": "20 pages, 10 figures, 2 tables To appear in Journal of Machine\n  Learning Research subject to minor revision",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.730926692485815,
      "community_id": null
    },
    {
      "id": "862266",
      "arxiv_id": "1706.07530",
      "title": "Multiresolution Match Kernels for Gesture Video Classification",
      "abstract": "The emergence of depth imaging technologies like the Microsoft Kinect has\nrenewed interest in computational methods for gesture classification based on\nvideos. For several years now, researchers have used the Bag-of-Features (BoF)\nas a primary method for generation of feature vectors from video data for\nrecognition of gestures. However, the BoF method is a coarse representation of\nthe information in a video, which often leads to poor similarity measures\nbetween videos. Besides, when features extracted from different spatio-temporal\nlocations in the video are pooled to create histogram vectors in the BoF\nmethod, there is an intrinsic loss of their original locations in space and\ntime. In this paper, we propose a new Multiresolution Match Kernel (MMK) for\nvideo classification, which can be considered as a generalization of the BoF\nmethod. We apply this procedure to hand gesture classification based on RGB-D\nvideos of the American Sign Language(ASL) hand gestures and our results show\npromise and usefulness of this new method.",
      "categories": "cs.CV",
      "update_date": "2017-06-26",
      "doi": null,
      "authors": "Hemanth Venkateswara, Vineeth N. Balasubramanian, Prasanth Lade,\n  Sethuraman Panchanathan",
      "comments": "ICME 2013 Conference",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.730837464332586,
      "community_id": null
    },
    {
      "id": "1915663",
      "arxiv_id": "2309.10858",
      "title": "On-device Real-time Custom Hand Gesture Recognition",
      "abstract": "Most existing hand gesture recognition (HGR) systems are limited to a\npredefined set of gestures. However, users and developers often want to\nrecognize new, unseen gestures. This is challenging due to the vast diversity\nof all plausible hand shapes, e.g. it is impossible for developers to include\nall hand gestures in a predefined list. In this paper, we present a\nuser-friendly framework that lets users easily customize and deploy their own\ngesture recognition pipeline. Our framework provides a pre-trained single-hand\nembedding model that can be fine-tuned for custom gesture recognition. Users\ncan perform gestures in front of a webcam to collect a small amount of images\nper gesture. We also offer a low-code solution to train and deploy the custom\ngesture recognition model. This makes it easy for users with limited ML\nexpertise to use our framework. We further provide a no-code web front-end for\nusers without any ML expertise. This makes it even easier to build and test the\nend-to-end pipeline. The resulting custom HGR is then ready to be run on-device\nfor real-time scenarios. This can be done by calling a simple function in our\nopen-sourced model inference API, MediaPipe Tasks. This entire process only\ntakes a few minutes.",
      "categories": "cs.CV",
      "update_date": "2023-09-21",
      "doi": null,
      "authors": "Esha Uboweja, David Tian, Qifei Wang, Yi-Chun Kuo, Joe Zou, Lu Wang,\n  George Sung, Matthias Grundmann",
      "comments": "5 pages, 6 figures; Accepted to ICCV Workshop on Computer Vision for\n  Metaverse, Paris, France, 2023",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.7307917636161,
      "community_id": "20241021_1FDMU"
    },
    {
      "id": "1515353",
      "arxiv_id": "2108.06537",
      "title": "Feature Identification and Matching for Hand Hygiene Pose",
      "abstract": "Three popular feature descriptors of computer vision such as SIFT, SURF, and\nORB compared and evaluated. The number of correct features extracted and\nmatched for the original hand hygiene pose-Rub hands palm to palm image and\nrotated image. An accuracy score calculated based on the total number of\nmatches and the correct number of matches produced. The experiment demonstrated\nthat ORB algorithm outperforms by giving the high number of correct matches in\nless amount of time. ORB feature detection technique applied over handwashing\nvideo recordings for feature extraction and hand hygiene pose classification as\na future work. OpenCV utilized to apply the algorithms within python scripts.",
      "categories": "cs.CV",
      "update_date": "2021-08-17",
      "doi": null,
      "authors": "Rashmi Bakshi",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.730598406159974,
      "community_id": null
    },
    {
      "id": "1320362",
      "arxiv_id": "2007.08847",
      "title": "Two-stream Fusion Model for Dynamic Hand Gesture Recognition using\n  3D-CNN and 2D-CNN Optical Flow guided Motion Template",
      "abstract": "The use of hand gestures can be a useful tool for many applications in the\nhuman-computer interaction community. In a broad range of areas hand gesture\ntechniques can be applied specifically in sign language recognition, robotic\nsurgery, etc. In the process of hand gesture recognition, proper detection, and\ntracking of the moving hand become challenging due to the varied shape and size\nof the hand. Here the objective is to track the movement of the hand\nirrespective of the shape, size, and color of the hand. And, for this, a motion\ntemplate guided by optical flow (OFMT) is proposed. OFMT is a compact\nrepresentation of the motion information of a gesture encoded into a single\nimage. In the experimentation, different datasets using bare hand with an open\npalm, and folded palm wearing green-glove are used, and in both cases, we could\ngenerate the OFMT images with equal precision. Recently, deep network-based\ntechniques have shown impressive improvements as compared to conventional\nhand-crafted feature-based techniques. Moreover, in the literature, it is seen\nthat the use of different streams with informative input data helps to increase\nthe performance in the recognition accuracy. This work basically proposes a\ntwo-stream fusion model for hand gesture recognition and a compact yet\nefficient motion template based on optical flow. Specifically, the two-stream\nnetwork consists of two layers: a 3D convolutional neural network (C3D) that\ntakes gesture videos as input and a 2D-CNN that takes OFMT images as input. C3D\nhas shown its efficiency in capturing spatio-temporal information of a video.\nWhereas OFMT helps to eliminate irrelevant gestures providing additional motion\ninformation. Though each stream can work independently, they are combined with\na fusion scheme to boost the recognition results. We have shown the efficiency\nof the proposed two-stream network on two databases.",
      "categories": "cs.CV eess.IV",
      "update_date": "2020-07-20",
      "doi": null,
      "authors": "Debajit Sarma, V. Kavyasree and M.K. Bhuyan",
      "comments": "7 pages, 6 figures, 2 tables. Keywords: Action and gesture\n  recognition, Two-stream fusion model, Optical flow guided motion template\n  (OFMT), 2D and 3D-CNN",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.730005280375167,
      "community_id": null
    },
    {
      "id": "2203888",
      "arxiv_id": "2412.01508",
      "title": "HaGRIDv2: 1M Images for Static and Dynamic Hand Gesture Recognition",
      "abstract": "This paper proposes the second version of the widespread Hand Gesture\nRecognition dataset HaGRID -- HaGRIDv2. We cover 15 new gestures with\nconversation and control functions, including two-handed ones. Building on the\nfoundational concepts proposed by HaGRID's authors, we implemented the dynamic\ngesture recognition algorithm and further enhanced it by adding three new\ngroups of manipulation gestures. The ``no gesture\" class was diversified by\nadding samples of natural hand movements, which allowed us to minimize false\npositives by 6 times. Combining extra samples with HaGRID, the received version\noutperforms the original in pre-training models for gesture-related tasks.\nBesides, we achieved the best generalization ability among gesture and hand\ndetection datasets. In addition, the second version enhances the quality of the\ngestures generated by the diffusion model. HaGRIDv2, pre-trained models, and a\ndynamic gesture recognition algorithm are publicly available.",
      "categories": "cs.CV",
      "update_date": "2024-12-03",
      "doi": null,
      "authors": "Anton Nuzhdin, Alexander Nagaev, Alexander Sautin, Alexander\n  Kapitanov, Karina Kvanchiani",
      "comments": "hand gesture recognition, dataset, hgr system, large-scale database",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.729621486090334,
      "community_id": "20241021_1ITH0"
    },
    {
      "id": "131591",
      "arxiv_id": "0906.5039",
      "title": "A new approach for digit recognition based on hand gesture analysis",
      "abstract": "We present in this paper a new approach for hand gesture analysis that allows\ndigit recognition. The analysis is based on extracting a set of features from a\nhand image and then combining them by using an induction graph. The most\nimportant features we extract from each image are the fingers locations, their\nheights and the distance between each pair of fingers. Our approach consists of\nthree steps: (i) Hand detection and localization, (ii) fingers extraction and\n(iii) features identification and combination to digit recognition. Each input\nimage is assumed to contain only one person, thus we apply a fuzzy classifier\nto identify the skin pixels. In the finger extraction step, we attempt to\nremove all the hand components except the fingers, this process is based on the\nhand anatomy properties. The final step consists on representing histogram of\nthe detected fingers in order to extract features that will be used for digit\nrecognition. The approach is invariant to scale, rotation and translation of\nthe hand. Some experiments have been undertaken to show the effectiveness of\nthe proposed approach.",
      "categories": "cs.CV",
      "update_date": "2009-06-30",
      "doi": null,
      "authors": "Ahmed Ben Jmaa, Walid Mahdi, Yousra Ben Jemaa, Abdelmajid Ben Hamadou",
      "comments": "8 Pages, International Journal of Computer Science and Information\n  Security",
      "journal_ref": "IJCSIS June 2009 Issue, Vol. 2, No. 1",
      "citation_count": null,
      "relevance": 0.728918644318742,
      "community_id": null
    },
    {
      "id": "1229101",
      "arxiv_id": "2001.03687",
      "title": "Recognition and Localisation of Pointing Gestures using a RGB-D Camera",
      "abstract": "Non-verbal communication is part of our regular conversation, and multiple\ngestures are used to exchange information. Among those gestures, pointing is\nthe most important one. If such gestures cannot be perceived by other team\nmembers, e.g. by blind and visually impaired people (BVIP), they lack important\ninformation and can hardly participate in a lively workflow. Thus, this paper\ndescribes a system for detecting such pointing gestures to provide input for\nsuitable output modalities to BVIP. Our system employs an RGB-D camera to\nrecognize the pointing gestures performed by the users. The system also locates\nthe target of pointing e.g. on a common workspace. We evaluated the system by\nconducting a user study with 26 users. The results show that the system has a\nsuccess rate of 89.59 and 79.92 % for a 2 x 3 matrix using the left and right\narm respectively, and 73.57 and 68.99 % for 3 x 4 matrix using the left and\nright arm respectively.",
      "categories": "cs.HC cs.RO eess.SP",
      "update_date": "2020-01-14",
      "doi": null,
      "authors": "Naina Dhingra, Eugenio Valli and Andreas Kunz",
      "comments": "8 pages, 5 figures",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.728509404405674,
      "community_id": null
    },
    {
      "id": "1536701",
      "arxiv_id": "2109.13879",
      "title": "3D Hand Pose and Shape Estimation from RGB Images for Keypoint-Based\n  Hand Gesture Recognition",
      "abstract": "Estimating the 3D pose of a hand from a 2D image is a well-studied problem\nand a requirement for several real-life applications such as virtual reality,\naugmented reality, and hand gesture recognition. Currently, reasonable\nestimations can be computed from single RGB images, especially when a\nmulti-task learning approach is used to force the system to consider the shape\nof the hand when its pose is determined. However, depending on the method used\nto represent the hand, the performance can drop considerably in real-life\ntasks, suggesting that stable descriptions are required to achieve satisfactory\nresults. In this paper, we present a keypoint-based end-to-end framework for 3D\nhand and pose estimation and successfully apply it to the task of hand gesture\nrecognition as a study case. Specifically, after a pre-processing step in which\nthe images are normalized, the proposed pipeline uses a multi-task semantic\nfeature extractor generating 2D heatmaps and hand silhouettes from RGB images,\na viewpoint encoder to predict the hand and camera view parameters, a stable\nhand estimator to produce the 3D hand pose and shape, and a loss function to\nguide all of the components jointly during the learning phase. Tests were\nperformed on a 3D pose and shape estimation benchmark dataset to assess the\nproposed framework, which obtained state-of-the-art performance. Our system was\nalso evaluated on two hand-gesture recognition benchmark datasets and\nsignificantly outperformed other keypoint-based approaches, indicating that it\nis an effective solution that is able to generate stable 3D estimates for hand\npose and shape.",
      "categories": "cs.CV",
      "update_date": "2022-05-10",
      "doi": "10.1016/j.patcog.2022.108762",
      "authors": "Danilo Avola, Luigi Cinque, Alessio Fagioli, Gian Luca Foresti,\n  Adriano Fragomeni, Daniele Pannone",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.728284510578572,
      "community_id": null
    },
    {
      "id": "1611344",
      "arxiv_id": "2202.12519",
      "title": "A Novel Hand Gesture Detection and Recognition system based on\n  ensemble-based Convolutional Neural Network",
      "abstract": "Nowadays, hand gesture recognition has become an alternative for\nhuman-machine interaction. It has covered a large area of applications like 3D\ngame technology, sign language interpreting, VR (virtual reality) environment,\nand robotics. But detection of the hand portion has become a challenging task\nin computer vision and pattern recognition communities. Deep learning algorithm\nlike convolutional neural network (CNN) architecture has become a very popular\nchoice for classification tasks, but CNN architectures suffer from some\nproblems like high variance during prediction, overfitting problem and also\nprediction errors. To overcome these problems, an ensemble of CNN-based\napproaches is presented in this paper. Firstly, the gesture portion is detected\nby using the background separation method based on binary thresholding. After\nthat, the contour portion is extracted, and the hand region is segmented. Then,\nthe images have been resized and fed into three individual CNN models to train\nthem in parallel. In the last part, the output scores of CNN models are\naveraged to construct an optimal ensemble model for the final prediction. Two\npublicly available datasets (labeled as Dataset-1 and Dataset-2) containing\ninfrared images and one self-constructed dataset have been used to validate the\nproposed system. Experimental results are compared with the existing\nstate-of-the-art approaches, and it is observed that our proposed ensemble\nmodel outperforms other existing proposed methods.",
      "categories": "cs.CV",
      "update_date": "2022-02-28",
      "doi": null,
      "authors": "Abir Sen, Tapas Kumar Mishra, Ratnakar Dash",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.727000149846059,
      "community_id": null
    },
    {
      "id": "1962201",
      "arxiv_id": "2312.00392",
      "title": "Study and Survey on Gesture Recognition Systems",
      "abstract": "In recent years, there has been a considerable amount of research in the\nGesture Recognition domain, mainly owing to the technological advancements in\nComputer Vision. Various new applications have been conceptualised and\ndeveloped in this field. This paper discusses the implementation of gesture\nrecognition systems in multiple sectors such as gaming, healthcare, home\nappliances, industrial robots, and virtual reality. Different methodologies for\ncapturing gestures are compared and contrasted throughout this survey. Various\ndata sources and data acquisition techniques have been discussed. The role of\ngestures in sign language has been studied and existing approaches have been\nreviewed. Common challenges faced while building gesture recognition systems\nhave also been explored.",
      "categories": "cs.CV",
      "update_date": "2023-12-04",
      "doi": null,
      "authors": "Kshitij Deshpande, Varad Mashalkar, Kaustubh Mhaisekar, Amaan Naikwadi\n  and Archana Ghotkar",
      "comments": "6 pages, accepted at the ICCUBEA, IEEE 2023 conference",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.726803319048685,
      "community_id": null
    },
    {
      "id": "1113546",
      "arxiv_id": "1904.08505",
      "title": "Dynamic Gesture Recognition by Using CNNs and Star RGB: a Temporal\n  Information Condensation",
      "abstract": "Due to the advance of technologies, machines are increasingly present in\npeople's daily lives. Thus, there has been more and more effort to develop\ninterfaces, such as dynamic gestures, that provide an intuitive way of\ninteraction. Currently, the most common trend is to use multimodal data, as\ndepth and skeleton information, to enable dynamic gesture recognition. However,\nusing only color information would be more interesting, since RGB cameras are\nusually available in almost every public place, and could be used for gesture\nrecognition without the need of installing other equipment. The main problem\nwith such approach is the difficulty of representing spatio-temporal\ninformation using just color. With this in mind, we propose a technique capable\nof condensing a dynamic gesture, shown in a video, in just one RGB image. We\ncall this technique star RGB. This image is then passed to a classifier formed\nby two Resnet CNNs, a soft-attention ensemble, and a fully connected layer,\nwhich indicates the class of the gesture present in the input video.\nExperiments were carried out using both Montalbano and GRIT datasets. For\nMontalbano dataset, the proposed approach achieved an accuracy of 94.58%. Such\nresult reaches the state-of-the-art when considering this dataset and only\ncolor information. Regarding the GRIT dataset, our proposal achieves more than\n98% of accuracy, recall, precision, and F1-score, outperforming the reference\napproach by more than 6%.",
      "categories": "cs.CV cs.HC cs.LG cs.RO",
      "update_date": "2020-06-19",
      "doi": "10.1016/j.neucom.2020.03.038",
      "authors": "Clebeson Canuto dos Santos, Jorge Leonid Aching Samatelo, Raquel\n  Frizera Vassallo",
      "comments": "19 pages, 12 figures, submitted to Neurocomputing Journal",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.726291391099526,
      "community_id": null
    },
    {
      "id": "2125619",
      "arxiv_id": "2408.05436",
      "title": "A Methodological and Structural Review of Hand Gesture Recognition\n  Across Diverse Data Modalities",
      "abstract": "Researchers have been developing Hand Gesture Recognition (HGR) systems to\nenhance natural, efficient, and authentic human-computer interaction,\nespecially benefiting those who rely solely on hand gestures for communication.\nDespite significant progress, the automatic and precise identification of hand\ngestures remains a considerable challenge in computer vision. Recent studies\nhave focused on specific modalities like RGB images, skeleton data, and\nspatiotemporal interest points. This paper provides a comprehensive review of\nHGR techniques and data modalities from 2014 to 2024, exploring advancements in\nsensor technology and computer vision. We highlight accomplishments using\nvarious modalities, including RGB, Skeleton, Depth, Audio, EMG, EEG, and\nMultimodal approaches and identify areas needing further research. We reviewed\nover 200 articles from prominent databases, focusing on data collection, data\nsettings, and gesture representation. Our review assesses the efficacy of HGR\nsystems through their recognition accuracy and identifies a gap in research on\ncontinuous gesture recognition, indicating the need for improved vision-based\ngesture systems. The field has experienced steady research progress, including\nadvancements in hand-crafted features and deep learning (DL) techniques.\nAdditionally, we report on the promising developments in HGR methods and the\narea of multimodal approaches. We hope this survey will serve as a potential\nguideline for diverse data modality-based HGR research.",
      "categories": "cs.CV",
      "update_date": "2024-09-12",
      "doi": "10.1109/ACCESS.2024.3456436",
      "authors": "Jungpil Shin, Abu Saleh Musa Miah, Md. Humaun Kabir, Md. Abdur Rahim,\n  Abdullah Al Shiam",
      "comments": null,
      "journal_ref": "IEEE Access-09 September 2024",
      "citation_count": null,
      "relevance": 0.724383907056815,
      "community_id": null
    },
    {
      "id": "1472623",
      "arxiv_id": "2105.10063",
      "title": "Uma implementa\\c{c}\\~ao do jogo Pedra, Papel e Tesoura utilizando Visao\n  Computacional",
      "abstract": "This paper presents a game, controlled by computer vision, in identification\nof hand gestures (hand-tracking). The proposed work is based on image\nsegmentation and construction of a convex hull with Jarvis Algorithm , and\ndetermination of the pattern based on the extraction of area characteristics in\nthe convex hull.",
      "categories": "cs.CV",
      "update_date": "2021-05-24",
      "doi": null,
      "authors": "Ezequiel Fran\\c{c}a dos Santos, Gabriel Fontenelle",
      "comments": "14 pages, in Portuguese",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.724006490468593,
      "community_id": null
    },
    {
      "id": "1042006",
      "arxiv_id": "1810.10818",
      "title": "HANDS18: Methods, Techniques and Applications for Hand Observation",
      "abstract": "This report outlines the proceedings of the Fourth International Workshop on\nObserving and Understanding Hands in Action (HANDS 2018). The fourth\ninstantiation of this workshop attracted significant interest from both\nacademia and the industry. The program of the workshop included regular papers\nthat are published as the workshop's proceedings, extended abstracts, invited\nposters, and invited talks. Topics of the submitted works and invited talks and\nposters included novel methods for hand pose estimation from RGB, depth, or\nskeletal data, datasets for special cases and real-world applications, and\ntechniques for hand motion re-targeting and hand gesture recognition. The\ninvited speakers are leaders in their respective areas of specialization,\ncoming from both industry and academia. The main conclusions that can be drawn\nare the turn of the community towards RGB data and the maturation of some\nmethods and techniques, which in turn has led to increasing interest for\nreal-world applications.",
      "categories": "cs.CV",
      "update_date": "2018-10-26",
      "doi": null,
      "authors": "Iason Oikonomidis, Guillermo Garcia-Hernando, Angela Yao, Antonis\n  Argyros, Vincent Lepetit, Tae-Kyun Kim",
      "comments": "11 pages, 1 figure, Discussion of the HANDS 2018 workshop held in\n  conjunction with ECCV 2018",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.723751501813443,
      "community_id": null
    },
    {
      "id": "1355461",
      "arxiv_id": "2009.13957",
      "title": "A Prototype-Based Generalized Zero-Shot Learning Framework for Hand\n  Gesture Recognition",
      "abstract": "Hand gesture recognition plays a significant role in human-computer\ninteraction for understanding various human gestures and their intent. However,\nmost prior works can only recognize gestures of limited labeled classes and\nfail to adapt to new categories. The task of Generalized Zero-Shot Learning\n(GZSL) for hand gesture recognition aims to address the above issue by\nleveraging semantic representations and detecting both seen and unseen class\nsamples. In this paper, we propose an end-to-end prototype-based GZSL framework\nfor hand gesture recognition which consists of two branches. The first branch\nis a prototype-based detector that learns gesture representations and\ndetermines whether an input sample belongs to a seen or unseen category. The\nsecond branch is a zero-shot label predictor which takes the features of unseen\nclasses as input and outputs predictions through a learned mapping mechanism\nbetween the feature and the semantic space. We further establish a hand gesture\ndataset that specifically targets this GZSL task, and comprehensive experiments\non this dataset demonstrate the effectiveness of our proposed approach on\nrecognizing both seen and unseen gestures.",
      "categories": "cs.CV",
      "update_date": "2020-09-30",
      "doi": null,
      "authors": "Jinting Wu, Yujia Zhang and Xiaoguang Zhao",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.723245534509953,
      "community_id": null
    },
    {
      "id": "296757",
      "arxiv_id": "1110.5450",
      "title": "Hand Tracking based on Hierarchical Clustering of Range Data",
      "abstract": "Fast and robust hand segmentation and tracking is an essential basis for\ngesture recognition and thus an important component for contact-less\nhuman-computer interaction (HCI). Hand gesture recognition based on 2D video\ndata has been intensively investigated. However, in practical scenarios purely\nintensity based approaches suffer from uncontrollable environmental conditions\nlike cluttered background colors. In this paper we present a real-time hand\nsegmentation and tracking algorithm using Time-of-Flight (ToF) range cameras\nand intensity data. The intensity and range information is fused into one pixel\nvalue, representing its combined intensity-depth homogeneity. The scene is\nhierarchically clustered using a GPU based parallel merging algorithm, allowing\na robust identification of both hands even for inhomogeneous backgrounds. After\nthe detection, both hands are tracked on the CPU. Our tracking algorithm can\ncope with the situation that one hand is temporarily covered by the other hand.",
      "categories": "cs.CV",
      "update_date": "2011-10-26",
      "doi": null,
      "authors": "Roberto Cespi, Andreas Kolb, Marvin Lindner",
      "comments": "Technical Report",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.721137838295828,
      "community_id": null
    },
    {
      "id": "520648",
      "arxiv_id": "1404.7594",
      "title": "Selecting a Small Set of Optimal Gestures from an Extensive Lexicon",
      "abstract": "Finding the best set of gestures to use for a given computer recognition\nproblem is an essential part of optimizing the recognition performance while\nbeing mindful to those who may articulate the gestures. An objective function,\ncalled the ellipsoidal distance ratio metric (EDRM), for determining the best\ngestures from a larger lexicon library is presented, along with a numerical\nmethod for incorporating subjective preferences. In particular, we demonstrate\nan efficient algorithm that chooses the best $n$ gestures from a lexicon of $m$\ngestures where typically $n \\ll m$ using a weighting of both subjective and\nobjective measures.",
      "categories": "cs.CV",
      "update_date": "2023-07-19",
      "doi": "10.5120/21060-3722",
      "authors": "Jacob Grosek and J. Nathan Kutz",
      "comments": "27 pages, 7 figures",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.720189170826128,
      "community_id": null
    },
    {
      "id": "546856",
      "arxiv_id": "1408.1549",
      "title": "Real-Time Human-Computer Interaction Based on Face and Hand Gesture\n  Recognition",
      "abstract": "At the present time, hand gestures recognition system could be used as a more\nexpected and useable approach for human computer interaction. Automatic hand\ngesture recognition system provides us a new tactic for interactive with the\nvirtual environment. In this paper, a face and hand gesture recognition system\nwhich is able to control computer media player is offered. Hand gesture and\nhuman face are the key element to interact with the smart system. We used the\nface recognition scheme for viewer verification and the hand gesture\nrecognition in mechanism of computer media player, for instance, volume\ndown/up, next music and etc. In the proposed technique, first, the hand gesture\nand face location is extracted from the main image by combination of skin and\ncascade detector and then is sent to recognition stage. In recognition stage,\nfirst, the threshold condition is inspected then the extracted face and gesture\nwill be recognized. In the result stage, the proposed technique is applied on\nthe video dataset and the high precision ratio acquired. Additional the\nrecommended hand gesture recognition method is applied on static American Sign\nLanguage (ASL) database and the correctness rate achieved nearby 99.40%. also\nthe planned method could be used in gesture based computer games and virtual\nreality.",
      "categories": "cs.CV",
      "update_date": "2014-08-08",
      "doi": "10.5121/ijfcst.2014.4403",
      "authors": "Reza Azad, Babak Azad, Nabil Belhaj Khalifa, Shahram Jamali",
      "comments": null,
      "journal_ref": "International Journal in Foundations of Computer Science &\n  Technology 07/2014; 4(4):37-48",
      "citation_count": null,
      "relevance": 0.719819026004135,
      "community_id": null
    },
    {
      "id": "911304",
      "arxiv_id": "1711.04293",
      "title": "Hand Gesture Recognition with Leap Motion",
      "abstract": "The recent introduction of depth cameras like Leap Motion Controller allows\nresearchers to exploit the depth information to recognize hand gesture more\nrobustly. This paper proposes a novel hand gesture recognition system with Leap\nMotion Controller. A series of features are extracted from Leap Motion tracking\ndata, we feed these features along with HOG feature extracted from sensor\nimages into a multi-class SVM classifier to recognize performed gesture,\ndimension reduction and feature weighted fusion are also discussed. Our results\nshow that our model is much more accurate than previous work.",
      "categories": "cs.CV",
      "update_date": "2017-11-15",
      "doi": null,
      "authors": "Youchen Du, Shenglan Liu, Lin Feng, Menghui Chen, Jie Wu",
      "comments": "6 pages, 10 figures",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.719512777357533,
      "community_id": "20241021_RPU8"
    },
    {
      "id": "1469703",
      "arxiv_id": "2105.07143",
      "title": "One for All: An End-to-End Compact Solution for Hand Gesture Recognition",
      "abstract": "The HGR is a quite challenging task as its performance is influenced by\nvarious aspects such as illumination variations, cluttered backgrounds,\nspontaneous capture, etc. The conventional CNN networks for HGR are following\ntwo stage pipeline to deal with the various challenges: complex signs,\nillumination variations, complex and cluttered backgrounds. The existing\napproaches needs expert expertise as well as auxiliary computation at stage 1\nto remove the complexities from the input images. Therefore, in this paper, we\nproposes an novel end-to-end compact CNN framework: fine grained feature\nattentive network for hand gesture recognition (Fit-Hand) to solve the\nchallenges as discussed above. The pipeline of the proposed architecture\nconsists of two main units: FineFeat module and dilated convolutional (Conv)\nlayer. The FineFeat module extracts fine grained feature maps by employing\nattention mechanism over multiscale receptive fields. The attention mechanism\nis introduced to capture effective features by enlarging the average behaviour\nof multi-scale responses. Moreover, dilated convolution provides global\nfeatures of hand gestures through a larger receptive field. In addition,\nintegrated layer is also utilized to combine the features of FineFeat module\nand dilated layer which enhances the discriminability of the network by\ncapturing complementary context information of hand postures. The effectiveness\nof Fit- Hand is evaluated by using subject dependent (SD) and subject\nindependent (SI) validation setup over seven benchmark datasets: MUGD-I,\nMUGD-II, MUGD-III, MUGD-IV, MUGD-V, Finger Spelling and OUHANDS, respectively.\nFurthermore, to investigate the deep insights of the proposed Fit-Hand\nframework, we performed ten ablation study.",
      "categories": "cs.CV",
      "update_date": "2021-05-18",
      "doi": null,
      "authors": "Monu Verma, Ayushi Gupta, santosh kumar Vipparthi",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.719295859336858,
      "community_id": null
    },
    {
      "id": "1144358",
      "arxiv_id": "1906.12171",
      "title": "Gesture Recognition in RGB Videos UsingHuman Body Keypoints and Dynamic\n  Time Warping",
      "abstract": "Gesture recognition opens up new ways for humans to intuitively interact with\nmachines. Especially for service robots, gestures can be a valuable addition to\nthe means of communication to, for example, draw the robot's attention to\nsomeone or something. Extracting a gesture from video data and classifying it\nis a challenging task and a variety of approaches have been proposed throughout\nthe years. This paper presents a method for gesture recognition in RGB videos\nusing OpenPose to extract the pose of a person and Dynamic Time Warping (DTW)\nin conjunction with One-Nearest-Neighbor (1NN) for time-series classification.\nThe main features of this approach are the independence of any specific\nhardware and high flexibility, because new gestures can be added to the\nclassifier by adding only a few examples of it. We utilize the robustness of\nthe Deep Learning-based OpenPose framework while avoiding the data-intensive\ntask of training a neural network ourselves. We demonstrate the classification\nperformance of our method using a public dataset.",
      "categories": "cs.CV cs.LG cs.RO",
      "update_date": "2019-07-01",
      "doi": null,
      "authors": "Pascal Schneider, Raphael Memmesheimer, Ivanna Kramer and Dietrich\n  Paulus",
      "comments": "13 pages, 4 figures, 2 tables, RoboCup 2019 Symposium",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.719242768332795,
      "community_id": null
    },
    {
      "id": "1093400",
      "arxiv_id": "1903.01013",
      "title": "Hand Pose Estimation: A Survey",
      "abstract": "The success of Deep Convolutional Neural Networks (CNNs) in recent years in\nalmost all the Computer Vision tasks on one hand, and the popularity of\nlow-cost consumer depth cameras on the other, has made Hand Pose Estimation a\nhot topic in computer vision field. In this report, we will first explain the\nhand pose estimation problem and will review major approaches solving this\nproblem, especially the two different problems of using depth maps or RGB\nimages. We will survey the most important papers in each field and will discuss\nthe strengths and weaknesses of each. Finally, we will explain the biggest\ndatasets in this field in detail and list 22 datasets with all their\nproperties. To the best of our knowledge this is the most complete list of all\nthe datasets in the hand pose estimation field.",
      "categories": "cs.CV",
      "update_date": "2019-06-04",
      "doi": null,
      "authors": "Bardia Doosti",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.719237957091575,
      "community_id": "20241021_HKFN"
    },
    {
      "id": "997049",
      "arxiv_id": "1806.11408",
      "title": "A Probabilistic Modeling Approach to One-Shot Gesture Recognition",
      "abstract": "Gesture recognition enables a natural extension of the way we currently\ninteract with devices. Commercially available gesture recognition systems are\nusually pre-trained and offer no option for customization by the user. In order\nto improve the user experience, it is desirable to allow end users to define\ntheir own gestures. This scenario requires learning from just a few training\nexamples if we want to impose only a light training load on the user. To this\nend, we propose a gesture classifier based on a hierarchical probabilistic\nmodeling approach. In this framework, high-level features that are shared among\ndifferent gestures can be extracted from a large labeled data set, yielding a\nprior distribution for gestures. When learning new types of gestures, the\nlearned shared prior reduces the number of required training examples for\nindividual gestures. We implemented the proposed gesture classifier for a Myo\nsensor bracelet and show favorable results for the tested system on a database\nof 17 different gesture types. Furthermore, we propose and implement two\nmethods to incorporate the gesture classifier in a real-time gesture\nrecognition system.",
      "categories": "eess.SP",
      "update_date": "2020-02-18",
      "doi": null,
      "authors": "Anouk van Diepen, Marco Cox and Bert de Vries",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.718607306480413,
      "community_id": null
    },
    {
      "id": "991294",
      "arxiv_id": "1806.05653",
      "title": "HGR-Net: A Fusion Network for Hand Gesture Segmentation and Recognition",
      "abstract": "We propose a two-stage convolutional neural network (CNN) architecture for\nrobust recognition of hand gestures, called HGR-Net, where the first stage\nperforms accurate semantic segmentation to determine hand regions, and the\nsecond stage identifies the gesture. The segmentation stage architecture is\nbased on the combination of fully convolutional residual network and atrous\nspatial pyramid pooling. Although the segmentation sub-network is trained\nwithout depth information, it is particularly robust against challenges such as\nillumination variations and complex backgrounds. The recognition stage deploys\na two-stream CNN, which fuses the information from the red-green-blue and\nsegmented images by combining their deep representations in a fully connected\nlayer before classification. Extensive experiments on public datasets show that\nour architecture achieves almost as good as state-of-the-art performance in\nsegmentation and recognition of static hand gestures, at a fraction of training\ntime, run time, and model size. Our method can operate at an average of 23 ms\nper frame.",
      "categories": "cs.CV",
      "update_date": "2020-01-01",
      "doi": null,
      "authors": "Amirhossein Dadashzadeh, Alireza Tavakoli Targhi, Maryam Tahmasbi,\n  Majid Mirmehdi",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.71853716859,
      "community_id": null
    },
    {
      "id": "1281930",
      "arxiv_id": "2005.02134",
      "title": "IPN Hand: A Video Dataset and Benchmark for Real-Time Continuous Hand\n  Gesture Recognition",
      "abstract": "In this paper, we introduce a new benchmark dataset named IPN Hand with\nsufficient size, variety, and real-world elements able to train and evaluate\ndeep neural networks. This dataset contains more than 4,000 gesture samples and\n800,000 RGB frames from 50 distinct subjects. We design 13 different static and\ndynamic gestures focused on interaction with touchless screens. We especially\nconsider the scenario when continuous gestures are performed without transition\nstates, and when subjects perform natural movements with their hands as\nnon-gesture actions. Gestures were collected from about 30 diverse scenes, with\nreal-world variation in background and illumination. With our dataset, the\nperformance of three 3D-CNN models is evaluated on the tasks of isolated and\ncontinuous real-time HGR. Furthermore, we analyze the possibility of increasing\nthe recognition accuracy by adding multiple modalities derived from RGB frames,\ni.e., optical flow and semantic segmentation, while keeping the real-time\nperformance of the 3D-CNN model. Our empirical study also provides a comparison\nwith the publicly available nvGesture (NVIDIA) dataset. The experimental\nresults show that the state-of-the-art ResNext-101 model decreases about 30%\naccuracy when using our real-world dataset, demonstrating that the IPN Hand\ndataset can be used as a benchmark, and may help the community to step forward\nin the continuous HGR. Our dataset and pre-trained models used in the\nevaluation are publicly available at https://github.com/GibranBenitez/IPN-hand.",
      "categories": "cs.CV",
      "update_date": "2020-10-21",
      "doi": null,
      "authors": "Gibran Benitez-Garcia, Jesus Olivares-Mercado, Gabriel Sanchez-Perez,\n  and Keiji Yanai",
      "comments": "Accepted at ICPR 2020",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.718135152085405,
      "community_id": null
    },
    {
      "id": "885594",
      "arxiv_id": "1709.00727",
      "title": "Hand Gesture Real Time Paint Tool - Box",
      "abstract": "With current development universally in computing, now a days user\ninteraction approaches with mouse, keyboard, touch-pens etc. are not\nsufficient. Directly using of hands or hand gestures as an input device is a\nmethod to attract people with providing the applications, through Machine\nLearning and Computer Vision. Human-computer interaction application in which\nyou can simply draw different shapes, fill the colors, moving the folder from\none place to another place and rotating your image with rotating your hand\ngesture all this will be without touching your device only. In this paper\nMachine Learning based hand gestures recognition is presented, with the use of\nComputer Vision different types of gesture applications have been created.",
      "categories": "cs.CV",
      "update_date": "2018-03-20",
      "doi": null,
      "authors": "Vandit Gajjar, Viraj Mavani, Ayesha Gurnani",
      "comments": "This paper needs a proper writing and experiments need to be\n  implemented, thus we are withdrawing this submission",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.717850923538213,
      "community_id": null
    },
    {
      "id": "798699",
      "arxiv_id": "1612.02889",
      "title": "Gesture-based Bootstrapping for Egocentric Hand Segmentation",
      "abstract": "Accurately identifying hands in images is a key sub-task for human activity\nunderstanding with wearable first-person point-of-view cameras. Traditional\nhand segmentation approaches rely on a large corpus of manually labeled data to\ngenerate robust hand detectors. However, these approaches still face challenges\nas the appearance of the hand varies greatly across users, tasks, environments\nor illumination conditions. A key observation in the case of many wearable\napplications and interfaces is that, it is only necessary to accurately detect\nthe user's hands in a specific situational context. Based on this observation,\nwe introduce an interactive approach to learn a person-specific hand\nsegmentation model that does not require any manually labeled training data.\nOur approach proceeds in two steps, an interactive bootstrapping step for\nidentifying moving hand regions, followed by learning a personalized user\nspecific hand appearance model. Concretely, our approach uses two convolutional\nneural networks: (1) a gesture network that uses pre-defined motion information\nto detect the hand region; and (2) an appearance network that learns a person\nspecific model of the hand region based on the output of the gesture network.\nDuring training, to make the appearance network robust to errors in the gesture\nnetwork, the loss function of the former network incorporates the confidence of\nthe gesture network while learning. Experiments demonstrate the robustness of\nour approach with an F1 score over 0.8 on all challenging datasets across a\nwide range of illumination and hand appearance variations, improving over a\nbaseline approach by over 10%.",
      "categories": "cs.CV",
      "update_date": "2018-06-18",
      "doi": null,
      "authors": "Yubo Zhang, Vishnu Naresh Boddeti, Kris M. Kitani",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.715544334254683,
      "community_id": null
    },
    {
      "id": "2090730",
      "arxiv_id": "2406.12440",
      "title": "Deep self-supervised learning with visualisation for automatic gesture\n  recognition",
      "abstract": "Gesture is an important mean of non-verbal communication, with visual\nmodality allows human to convey information during interaction, facilitating\npeoples and human-machine interactions. However, it is considered difficult to\nautomatically recognise gestures. In this work, we explore three different\nmeans to recognise hand signs using deep learning: supervised learning based\nmethods, self-supervised methods and visualisation based techniques applied to\n3D moving skeleton data. Self-supervised learning used to train fully\nconnected, CNN and LSTM method. Then, reconstruction method is applied to\nunlabelled data in simulated settings using CNN as a backbone where we use the\nlearnt features to perform the prediction in the remaining labelled data.\nLastly, Grad-CAM is applied to discover the focus of the models. Our\nexperiments results show that supervised learning method is capable to\nrecognise gesture accurately, with self-supervised learning increasing the\naccuracy in simulated settings. Finally, Grad-CAM visualisation shows that\nindeed the models focus on relevant skeleton joints on the associated gesture.",
      "categories": "cs.CV cs.HC",
      "update_date": "2024-06-19",
      "doi": null,
      "authors": "Fabien Allemand, Alessio Mazzela, Jun Villette, Decky Aspandi, Titus\n  Zaharia",
      "comments": "Student research project with company collaboration",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.715341904302186,
      "community_id": null
    },
    {
      "id": "1682605",
      "arxiv_id": "2207.06706",
      "title": "SHREC 2022 Track on Online Detection of Heterogeneous Gestures",
      "abstract": "This paper presents the outcomes of a contest organized to evaluate methods\nfor the online recognition of heterogeneous gestures from sequences of 3D hand\nposes. The task is the detection of gestures belonging to a dictionary of 16\nclasses characterized by different pose and motion features. The dataset\nfeatures continuous sequences of hand tracking data where the gestures are\ninterleaved with non-significant motions. The data have been captured using the\nHololens 2 finger tracking system in a realistic use-case of mixed reality\ninteraction. The evaluation is based not only on the detection performances but\nalso on the latency and the false positives, making it possible to understand\nthe feasibility of practical interaction tools based on the algorithms\nproposed. The outcomes of the contest's evaluation demonstrate the necessity of\nfurther research to reduce recognition errors, while the computational cost of\nthe algorithms proposed is sufficiently low.",
      "categories": "cs.CV",
      "update_date": "2022-07-25",
      "doi": null,
      "authors": "Ariel Caputo, Marco Emporio, Andrea Giachetti, Marco Cristani, Guido\n  Borghi, Andrea D'Eusanio, Minh-Quan Le, Hai-Dang Nguyen, Minh-Triet Tran, F.\n  Ambellan, M. Hanik, E. Nava-Yazdani, C. von Tycowicz",
      "comments": "Accepted on Computer & Graphics journal",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.715093272024795,
      "community_id": null
    },
    {
      "id": "1679011",
      "arxiv_id": "2207.03112",
      "title": "Deep learning based Hand gesture recognition system and design of a\n  Human-Machine Interface",
      "abstract": "In this work, a real-time hand gesture recognition system-based\nhuman-computer interface (HCI) is presented. The system consists of six stages:\n(1) hand detection, (2) gesture segmentation, (3) use of five pre-trained\nconvolutional neural network models (CNN) and vision transformer (ViT), (4)\nbuilding an interactive human-machine interface (HMI), (5) development of a\ngesture-controlled virtual mouse, (6) use of Kalman filter to estimate the hand\nposition, based on that the smoothness of the motion of pointer is improved. In\nour work, five pre-trained CNN (VGG16, VGG19, ResNet50, ResNet101, and\nInception-V1) models and ViT have been employed to classify hand gesture\nimages. Two multi-class datasets (one public and one custom) have been used to\nvalidate the models. Considering the model's performances, it is observed that\nInception-V1 has significantly shown a better classification performance\ncompared to the other four CNN models and ViT in terms of accuracy, precision,\nrecall, and F-score values. We have also expanded this system to control some\ndesktop applications (such as VLC player, audio player, file management,\nplaying 2D Super-Mario-Bros game, etc.) with different customized gesture\ncommands in real-time scenarios. The average speed of this system has reached\n25 fps (frames per second), which meets the requirements for the real-time\nscenario. Performance of the proposed gesture control system obtained the\naverage response time in milisecond for each control which makes it suitable\nfor real-time. This model (prototype) will benefit physically disabled people\ninteracting with desktops.",
      "categories": "cs.CV cs.HC",
      "update_date": "2023-01-18",
      "doi": null,
      "authors": "Abir Sen, Tapas Kumar Mishra and Ratnakar Dash",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.714674668629489,
      "community_id": "20241021_PM2H"
    },
    {
      "id": "554476",
      "arxiv_id": "1409.2050",
      "title": "Depth image hand tracking from an overhead perspective using partially\n  labeled, unbalanced data: Development and real-world testing",
      "abstract": "We present the development and evaluation of a hand tracking algorithm based\non single depth images captured from an overhead perspective for use in the\nCOACH prompting system. We train a random decision forest body part classifier\nusing approximately 5,000 manually labeled, unbalanced, partially labeled\ntraining images. The classifier represents a random subset of pixels in each\ndepth image with a learned probability density function across all trained body\nparts. A local mode-find approach is used to search for clusters present in the\nunderlying feature space sampled by the classified pixels. In each frame, body\npart positions are chosen as the mode with the highest confidence. User hand\npositions are translated into hand washing task actions based on proximity to\nenvironmental objects. We validate the performance of the classifier and task\naction proposals on a large set of approximately 24,000 manually labeled\nimages.",
      "categories": "cs.CV",
      "update_date": "2015-03-10",
      "doi": "10.3109/17483107.2015.1027304",
      "authors": "Stephen Czarnuch, Alex Mihailidis",
      "comments": "9 pages, 1 figure",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.713897977602723,
      "community_id": null
    },
    {
      "id": "2100974",
      "arxiv_id": "2407.02585",
      "title": "Novel Human Machine Interface via Robust Hand Gesture Recognition System\n  using Channel Pruned YOLOv5s Model",
      "abstract": "Hand gesture recognition (HGR) is a vital component in enhancing the\nhuman-computer interaction experience, particularly in multimedia applications,\nsuch as virtual reality, gaming, smart home automation systems, etc. Users can\ncontrol and navigate through these applications seamlessly by accurately\ndetecting and recognizing gestures. However, in a real-time scenario, the\nperformance of the gesture recognition system is sometimes affected due to the\npresence of complex background, low-light illumination, occlusion problems,\netc. Another issue is building a fast and robust gesture-controlled\nhuman-computer interface (HCI) in the real-time scenario. The overall objective\nof this paper is to develop an efficient hand gesture detection and\nclassification model using a channel-pruned YOLOv5-small model and utilize the\nmodel to build a gesture-controlled HCI with a quick response time (in ms) and\nhigher detection speed (in fps). First, the YOLOv5s model is chosen for the\ngesture detection task. Next, the model is simplified by using a channel-pruned\nalgorithm. After that, the pruned model is further fine-tuned to ensure\ndetection efficiency. We have compared our suggested scheme with other\nstate-of-the-art works, and it is observed that our model has shown superior\nresults in terms of mAP (mean average precision), precision (\\%), recall (\\%),\nand F1-score (\\%), fast inference time (in ms), and detection speed (in fps).\nOur proposed method paves the way for deploying a pruned YOLOv5s model for a\nreal-time gesture-command-based HCI to control some applications, such as the\nVLC media player, Spotify player, etc., using correctly classified gesture\ncommands in real-time scenarios. The average detection speed of our proposed\nsystem has reached more than 60 frames per second (fps) in real-time, which\nmeets the perfect requirement in real-time application control.",
      "categories": "cs.CV",
      "update_date": "2024-07-04",
      "doi": null,
      "authors": "Abir Sen, Tapas Kumar Mishra and Ratnakar Dash",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.713762453717748,
      "community_id": null
    },
    {
      "id": "1122662",
      "arxiv_id": "1905.04225",
      "title": "Talking With Your Hands: Scaling Hand Gestures and Recognition With CNNs",
      "abstract": "The use of hand gestures provides a natural alternative to cumbersome\ninterface devices for Human-Computer Interaction (HCI) systems. As the\ntechnology advances and communication between humans and machines becomes more\ncomplex, HCI systems should also be scaled accordingly in order to accommodate\nthe introduced complexities. In this paper, we propose a methodology to scale\nhand gestures by forming them with predefined gesture-phonemes, and a\nconvolutional neural network (CNN) based framework to recognize hand gestures\nby learning only their constituents of gesture-phonemes. The total number of\npossible hand gestures can be increased exponentially by increasing the number\nof used gesture-phonemes. For this objective, we introduce a new benchmark\ndataset named Scaled Hand Gestures Dataset (SHGD) with only gesture-phonemes in\nits training set and 3-tuples gestures in the test set. In our experimental\nanalysis, we achieve to recognize hand gestures containing one and three\ngesture-phonemes with an accuracy of 98.47% (in 15 classes) and 94.69% (in 810\nclasses), respectively. Our dataset, code and pretrained models are publicly\navailable.",
      "categories": "cs.CV cs.HC cs.LG",
      "update_date": "2019-09-02",
      "doi": null,
      "authors": "Okan K\\\"op\\\"ukl\\\"u, Yao Rong, Gerhard Rigoll",
      "comments": "Accepted to ICCV 2019 workshop - Observing and Understanding Hands in\n  Action (HANDS 2019)",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.713667946239238,
      "community_id": null
    },
    {
      "id": "1176711",
      "arxiv_id": "1909.06672",
      "title": "Progression Modelling for Online and Early Gesture Detection",
      "abstract": "Online and Early detection of gestures is crucial for building touchless\ngesture based interfaces. These interfaces should operate on a stream of video\nframes instead of the complete video and detect the presence of gestures at an\nearlier stage than post-completion for providing real time user experience. To\nachieve this, it is important to recognize the progression of the gesture\nacross different stages so that appropriate responses can be triggered on\nreaching the desired execution stage. To address this, we propose a simple yet\neffective multi-task learning framework which models the progression of the\ngesture along with frame level recognition. The proposed framework recognizes\nthe gestures at an early stage with high precision and also achieves\nstate-of-the-art recognition accuracy of 87.8% which is closer to human\naccuracy of 88.4% on the NVIDIA gesture dataset in the offline configuration\nand advances the state-of-the-art by more than 4%. We also introduce tightly\nsegmented annotations for the NVIDIA gesture dataset and setup a strong\nbaseline for gesture localization for this dataset. We also evaluate our\nframework on the Montalbano dataset and report competitive results.",
      "categories": "cs.CV",
      "update_date": "2019-09-17",
      "doi": null,
      "authors": "Vikram Gupta, Sai Kumar Dwivedi, Rishabh Dabral, Arjun Jain",
      "comments": "3DV 2019 Oral paper",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.712936418477512,
      "community_id": null
    },
    {
      "id": "1251806",
      "arxiv_id": "2003.01450",
      "title": "3D dynamic hand gestures recognition using the Leap Motion sensor and\n  convolutional neural networks",
      "abstract": "Defining methods for the automatic understanding of gestures is of paramount\nimportance in many application contexts and in Virtual Reality applications for\ncreating more natural and easy-to-use human-computer interaction methods. In\nthis paper, we present a method for the recognition of a set of non-static\ngestures acquired through the Leap Motion sensor. The acquired gesture\ninformation is converted in color images, where the variation of hand joint\npositions during the gesture are projected on a plane and temporal information\nis represented with color intensity of the projected points. The classification\nof the gestures is performed using a deep Convolutional Neural Network (CNN). A\nmodified version of the popular ResNet-50 architecture is adopted, obtained by\nremoving the last fully connected layer and adding a new layer with as many\nneurons as the considered gesture classes. The method has been successfully\napplied to the existing reference dataset and preliminary tests have already\nbeen performed for the real-time recognition of dynamic gestures performed by\nusers.",
      "categories": "cs.CV cs.LG eess.IV",
      "update_date": "2020-09-03",
      "doi": null,
      "authors": "Katia Lupinetti, Andrea Ranieri, Franca Giannini, Marina Monti",
      "comments": "Conference paper, 19 pages. UPDATE 20200311: changed in ref [19]\n  'International Journal of Engineering and Technology' -\u003E ' International\n  Journal of Engineering and Technology Innovation'. UPDATE 20200902: changed\n  every LMHGD occurrence to LMDHG",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.712711172538331,
      "community_id": null
    },
    {
      "id": "867270",
      "arxiv_id": "1707.02237",
      "title": "The 2017 Hands in the Million Challenge on 3D Hand Pose Estimation",
      "abstract": "We present the 2017 Hands in the Million Challenge, a public competition\ndesigned for the evaluation of the task of 3D hand pose estimation. The goal of\nthis challenge is to assess how far is the state of the art in terms of solving\nthe problem of 3D hand pose estimation as well as detect major failure and\nstrength modes of both systems and evaluation metrics that can help to identify\nfuture research directions. The challenge follows up the recent publication of\nBigHand2.2M and First-Person Hand Action datasets, which have been designed to\nexhaustively cover multiple hand, viewpoint, hand articulation, and occlusion.\nThe challenge consists of a standardized dataset, an evaluation protocol for\ntwo different tasks, and a public competition. In this document we describe the\ndifferent aspects of the challenge and, jointly with the results of the\nparticipants, it will be presented at the 3rd International Workshop on\nObserving and Understanding Hands in Action, HANDS 2017, with ICCV 2017.",
      "categories": "cs.CV",
      "update_date": "2017-07-10",
      "doi": null,
      "authors": "Shanxin Yuan, Qi Ye, Guillermo Garcia-Hernando, Tae-Kyun Kim",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.71268530412404,
      "community_id": null
    },
    {
      "id": "1668640",
      "arxiv_id": "2206.08219",
      "title": "HaGRID - HAnd Gesture Recognition Image Dataset",
      "abstract": "This paper introduces an enormous dataset, HaGRID (HAnd Gesture Recognition\nImage Dataset), to build a hand gesture recognition (HGR) system concentrating\non interaction with devices to manage them. That is why all 18 chosen gestures\nare endowed with the semiotic function and can be interpreted as a specific\naction. Although the gestures are static, they were picked up, especially for\nthe ability to design several dynamic gestures. It allows the trained model to\nrecognize not only static gestures such as \"like\" and \"stop\" but also \"swipes\"\nand \"drag and drop\" dynamic gestures. The HaGRID contains 554,800 images and\nbounding box annotations with gesture labels to solve hand detection and\ngesture classification tasks. The low variability in context and subjects of\nother datasets was the reason for creating the dataset without such\nlimitations. Utilizing crowdsourcing platforms allowed us to collect samples\nrecorded by 37,583 subjects in at least as many scenes with subject-to-camera\ndistances from 0.5 to 4 meters in various natural light conditions. The\ninfluence of the diversity characteristics was assessed in ablation study\nexperiments. Also, we demonstrate the HaGRID ability to be used for pretraining\nmodels in HGR tasks. The HaGRID and pretrained models are publicly available.",
      "categories": "cs.CV",
      "update_date": "2024-10-10",
      "doi": "10.1109/WACV57701.2024.00451",
      "authors": "Alexander Kapitanov, Karina Kvanchiani, Alexander Nagaev, Roman\n  Kraynov, Andrei Makhliarchuk",
      "comments": "12 pages, 5 figures, open-source dataset for computer vision",
      "journal_ref": "Proceedings of the IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV) (2024) 4572-4581",
      "citation_count": null,
      "relevance": 0.712087073561937,
      "community_id": "20241021_1ITH0"
    },
    {
      "id": "821966",
      "arxiv_id": "1702.07371",
      "title": "Feasibility of Principal Component Analysis in hand gesture recognition\n  system",
      "abstract": "Nowadays actions are increasingly being handled in electronic ways, instead\nof physical interaction. From earlier times biometrics is used in the\nauthentication of a person. It recognizes a person by using a human trait\nassociated with it like eyes (by calculating the distance between the eyes) and\nusing hand gestures, fingerprint detection, face detection etc. Advantages of\nusing these traits for identification are that they uniquely identify a person\nand cannot be forgotten or lost. These are unique features of a human being\nwhich are being used widely to make the human life simpler. Hand gesture\nrecognition system is a powerful tool that supports efficient interaction\nbetween the user and the computer. The main moto of hand gesture recognition\nresearch is to create a system which can recognise specific hand gestures and\nuse them to convey useful information for device control. This paper presents\nan experimental study over the feasibility of principal component analysis in\nhand gesture recognition system. PCA is a powerful tool for analyzing data. The\nprimary goal of PCA is dimensionality reduction. Frames are extracted from the\nSheffield KInect Gesture (SKIG) dataset. The implementation is done by creating\na training set and then training the recognizer. It uses Eigen space by\nprocessing the eigenvalues and eigenvectors of the images in training set.\nEuclidean distance with the threshold value is used as similarity metric to\nrecognize the gestures. The experimental results show that PCA is feasible to\nbe used for hand gesture recognition system.",
      "categories": "cs.CV",
      "update_date": "2017-02-27",
      "doi": null,
      "authors": "Tanu Srivastava, Raj Shree Singh, Sunil Kumar, Pavan Chakraborty",
      "comments": "conference",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.711851396803519,
      "community_id": null
    },
    {
      "id": "887647",
      "arxiv_id": "1709.02780",
      "title": "Detecting Hands in Egocentric Videos: Towards Action Recognition",
      "abstract": "Recently, there has been a growing interest in analyzing human daily\nactivities from data collected by wearable cameras. Since the hands are\ninvolved in a vast set of daily tasks, detecting hands in egocentric images is\nan important step towards the recognition of a variety of egocentric actions.\nHowever, besides extreme illumination changes in egocentric images, hand\ndetection is not a trivial task because of the intrinsic large variability of\nhand appearance. We propose a hand detector that exploits skin modeling for\nfast hand proposal generation and Convolutional Neural Networks for hand\nrecognition. We tested our method on UNIGE-HANDS dataset and we showed that the\nproposed approach achieves competitive hand detection results.",
      "categories": "cs.CV",
      "update_date": "2017-09-11",
      "doi": null,
      "authors": "Alejandro Cartas, Mariella Dimiccoli and Petia Radeva",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.71165521671443,
      "community_id": "20241021_HKFN"
    },
    {
      "id": "1332360",
      "arxiv_id": "2008.04637",
      "title": "Real-Time Sign Language Detection using Human Pose Estimation",
      "abstract": "We propose a lightweight real-time sign language detection model, as we\nidentify the need for such a case in videoconferencing. We extract optical flow\nfeatures based on human pose estimation and, using a linear classifier, show\nthese features are meaningful with an accuracy of 80%, evaluated on the DGS\nCorpus. Using a recurrent model directly on the input, we see improvements of\nup to 91% accuracy, while still working under 4ms. We describe a demo\napplication to sign language detection in the browser in order to demonstrate\nits usage possibility in videoconferencing applications.",
      "categories": "cs.CV cs.CL",
      "update_date": "2020-09-15",
      "doi": null,
      "authors": "Amit Moryossef, Ioannis Tsochantaridis, Roee Aharoni, Sarah Ebling,\n  and Srini Narayanan",
      "comments": "10 pages",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.71146948865238,
      "community_id": null
    },
    {
      "id": "161582",
      "arxiv_id": "0912.1830",
      "title": "Gesture Recognition with a Focus on Important Actions by Using a Path\n  Searching Method in Weighted Graph",
      "abstract": "This paper proposes a method of gesture recognition with a focus on important\nactions for distinguishing similar gestures. The method generates a partial\naction sequence by using optical flow images, expresses the sequence in the\neigenspace, and checks the feature vector sequence by applying an optimum\npath-searching method of weighted graph to focus the important actions. Also\npresented are the results of an experiment on the recognition of similar sign\nlanguage words.",
      "categories": "cs.CV cs.LG",
      "update_date": "2009-12-10",
      "doi": null,
      "authors": "Kazumoto Tanaka",
      "comments": "International Journal of Computer Science Issues, IJCSI Volume 6,\n  Issue 2, pp14-19, November 2009",
      "journal_ref": "K. TANAKA, \"Gesture Recognition with a Focus on Important Actions\n  by Using a Path Searching Method in Weighted Graph\", International Journal of\n  Computer Science Issues, IJCSI, Volume 6, Issue 2, pp14-19, November 2009",
      "citation_count": null,
      "relevance": 0.711123620137737,
      "community_id": null
    },
    {
      "id": "1824366",
      "arxiv_id": "2304.06319",
      "title": "Continual Learning of Hand Gestures for Human-Robot Interaction",
      "abstract": "In this paper, we present an efficient method to incrementally learn to\nclassify static hand gestures. This method allows users to teach a robot to\nrecognize new symbols in an incremental manner. Contrary to other works which\nuse special sensors or external devices such as color or data gloves, our\nproposed approach makes use of a single RGB camera to perform static hand\ngesture recognition from 2D images. Furthermore, our system is able to\nincrementally learn up to 38 new symbols using only 5 samples for each old\nclass, achieving a final average accuracy of over 90\\%. In addition to that,\nthe incremental training time can be reduced to a 10\\% of the time required\nwhen using all data available.",
      "categories": "cs.RO",
      "update_date": "2023-04-14",
      "doi": null,
      "authors": "Xavier Cucurull and Ana\\'is Garrell",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.711083608205105,
      "community_id": null
    },
    {
      "id": "753816",
      "arxiv_id": "1607.06264",
      "title": "Left/Right Hand Segmentation in Egocentric Videos",
      "abstract": "Wearable cameras allow people to record their daily activities from a\nuser-centered (First Person Vision) perspective. Due to their favorable\nlocation, wearable cameras frequently capture the hands of the user, and may\nthus represent a promising user-machine interaction tool for different\napplications. Existent First Person Vision methods handle hand segmentation as\na background-foreground problem, ignoring two important facts: i) hands are not\na single \"skin-like\" moving element, but a pair of interacting cooperative\nentities, ii) close hand interactions may lead to hand-to-hand occlusions and,\nas a consequence, create a single hand-like segment. These facts complicate a\nproper understanding of hand movements and interactions. Our approach extends\ntraditional background-foreground strategies, by including a\nhand-identification step (left-right) based on a Maxwell distribution of angle\nand position. Hand-to-hand occlusions are addressed by exploiting temporal\nsuperpixels. The experimental results show that, in addition to a reliable\nleft/right hand-segmentation, our approach considerably improves the\ntraditional background-foreground hand-segmentation.",
      "categories": "cs.HC cs.AI cs.CV",
      "update_date": "2016-09-15",
      "doi": "10.1016/j.cviu.2016.09.005",
      "authors": "Alejandro Betancourt, Pietro Morerio, Emilia Barakova, Lucio\n  Marcenaro, Matthias Rauterberg, Carlo Regazzoni",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.710993068755087,
      "community_id": null
    },
    {
      "id": "1461768",
      "arxiv_id": "2104.14349",
      "title": "Hand Gesture Recognition Based on a Nonconvex Regularization",
      "abstract": "Recognition of hand gestures is one of the most fundamental tasks in\nhuman-robot interaction. Sparse representation based methods have been widely\nused due to their efficiency and low demands on the training data. Recently,\nnonconvex regularization techniques including the $\\ell_{1-2}$ regularization\nhave been proposed in the image processing community to promote sparsity while\nachieving efficient performance. In this paper, we propose a vision-based hand\ngesture recognition model based on the $\\ell_{1-2}$ regularization, which is\nsolved by the alternating direction method of multipliers (ADMM). Numerical\nexperiments on binary and gray-scale data sets have demonstrated the\neffectiveness of this method in identifying hand gestures.",
      "categories": "cs.CV cs.LG math.OC",
      "update_date": "2022-04-27",
      "doi": "10.1109/ICMA52036.2021.9512752",
      "authors": "Jing Qin and Joshua Ashley and Biyun Xie",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.710942064411394,
      "community_id": null
    },
    {
      "id": "1801345",
      "arxiv_id": "2303.01547",
      "title": "Simultaneous prediction of hand gestures, handedness, and hand keypoints\n  using thermal images",
      "abstract": "Hand gesture detection is a well-explored area in computer vision with\napplications in various forms of Human-Computer Interactions. In this work, we\npropose a technique for simultaneous hand gesture classification, handedness\ndetection, and hand keypoints localization using thermal data captured by an\ninfrared camera. Our method uses a novel deep multi-task learning architecture\nthat includes shared encoderdecoder layers followed by three branches dedicated\nfor each mentioned task. We performed extensive experimental validation of our\nmodel on an in-house dataset consisting of 24 users data. The results confirm\nhigher than 98 percent accuracy for gesture classification, handedness\ndetection, and fingertips localization, and more than 91 percent accuracy for\nwrist points localization.",
      "categories": "cs.CV cs.AI",
      "update_date": "2023-03-06",
      "doi": null,
      "authors": "Sichao Li, Sean Banerjee, Natasha Kholgade Banerjee, Soumyabrata Dey",
      "comments": "ICDEC 2022",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.71036896972749,
      "community_id": null
    },
    {
      "id": "1233461",
      "arxiv_id": "2001.08047",
      "title": "Attention! A Lightweight 2D Hand Pose Estimation Approach",
      "abstract": "Vision based human pose estimation is an non-invasive technology for\nHuman-Computer Interaction (HCI). Direct use of the hand as an input device\nprovides an attractive interaction method, with no need for specialized sensing\nequipment, such as exoskeletons, gloves etc, but a camera. Traditionally, HCI\nis employed in various applications spreading in areas including manufacturing,\nsurgery, entertainment industry and architecture, to mention a few. Deployment\nof vision based human pose estimation algorithms can give a breath of\ninnovation to these applications. In this letter, we present a novel\nConvolutional Neural Network architecture, reinforced with a Self-Attention\nmodule that it can be deployed on an embedded system, due to its lightweight\nnature, with just 1.9 Million parameters. The source code and qualitative\nresults are publicly available.",
      "categories": "cs.CV cs.HC cs.LG",
      "update_date": "2020-06-02",
      "doi": null,
      "authors": "Nicholas Santavas, Ioannis Kansizoglou, Loukas Bampis, Evangelos\n  Karakasis and Antonios Gasteratos",
      "comments": "updated version with ablation studies",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.71018830416342,
      "community_id": null
    },
    {
      "id": "731505",
      "arxiv_id": "1605.03389",
      "title": "Efficiently Creating 3D Training Data for Fine Hand Pose Estimation",
      "abstract": "While many recent hand pose estimation methods critically rely on a training\nset of labelled frames, the creation of such a dataset is a challenging task\nthat has been overlooked so far. As a result, existing datasets are limited to\na few sequences and individuals, with limited accuracy, and this prevents these\nmethods from delivering their full potential. We propose a semi-automated\nmethod for efficiently and accurately labeling each frame of a hand depth video\nwith the corresponding 3D locations of the joints: The user is asked to provide\nonly an estimate of the 2D reprojections of the visible joints in some\nreference frames, which are automatically selected to minimize the labeling\nwork by efficiently optimizing a sub-modular loss function. We then exploit\nspatial, temporal, and appearance constraints to retrieve the full 3D poses of\nthe hand over the complete sequence. We show that this data can be used to\ntrain a recent state-of-the-art hand pose estimation method, leading to\nincreased accuracy. The code and dataset can be found on our website\nhttps://cvarlab.icg.tugraz.at/projects/hand_detection/",
      "categories": "cs.CV cs.HC",
      "update_date": "2016-12-05",
      "doi": null,
      "authors": "Markus Oberweger, Gernot Riegler, Paul Wohlhart, Vincent Lepetit",
      "comments": "added link to source https://github.com/moberweger/semi-auto-anno.\n  Appears in Proc. of CVPR 2016",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.710169749845879,
      "community_id": "20241021_HKFN"
    },
    {
      "id": "1519786",
      "arxiv_id": "2108.10970",
      "title": "Real-time Indian Sign Language (ISL) Recognition",
      "abstract": "This paper presents a system which can recognise hand poses & gestures from\nthe Indian Sign Language (ISL) in real-time using grid-based features. This\nsystem attempts to bridge the communication gap between the hearing and speech\nimpaired and the rest of the society. The existing solutions either provide\nrelatively low accuracy or do not work in real-time. This system provides good\nresults on both the parameters. It can identify 33 hand poses and some gestures\nfrom the ISL. Sign Language is captured from a smartphone camera and its frames\nare transmitted to a remote server for processing. The use of any external\nhardware (such as gloves or the Microsoft Kinect sensor) is avoided, making it\nuser-friendly. Techniques such as Face detection, Object stabilisation and Skin\nColour Segmentation are used for hand detection and tracking. The image is\nfurther subjected to a Grid-based Feature Extraction technique which represents\nthe hand's pose in the form of a Feature Vector. Hand poses are then classified\nusing the k-Nearest Neighbours algorithm. On the other hand, for gesture\nclassification, the motion and intermediate hand poses observation sequences\nare fed to Hidden Markov Model chains corresponding to the 12 pre-selected\ngestures defined in ISL. Using this methodology, the system is able to achieve\nan accuracy of 99.7% for static hand poses, and an accuracy of 97.23% for\ngesture recognition.",
      "categories": "cs.CV cs.HC",
      "update_date": "2021-08-26",
      "doi": null,
      "authors": "Kartik Shenoy, Tejas Dastane, Varun Rao, Devendra Vyavaharkar",
      "comments": "9 pages",
      "journal_ref": "9th International Conference on Communication and Network\n  Technology 2018",
      "citation_count": null,
      "relevance": 0.710007446407829,
      "community_id": null
    },
    {
      "id": "1413988",
      "arxiv_id": "2101.10245",
      "title": "AirWare: Utilizing Embedded Audio and Infrared Signals for In-Air\n  Hand-Gesture Recognition",
      "abstract": "We introduce AirWare, an in-air hand-gesture recognition system that uses the\nalready embedded speaker and microphone in most electronic devices, together\nwith embedded infrared proximity sensors. Gestures identified by AirWare are\nperformed in the air above a touchscreen or a mobile phone. AirWare utilizes\nconvolutional neural networks to classify a large vocabulary of hand gestures\nusing multi-modal audio Doppler signatures and infrared (IR) sensor\ninformation. As opposed to other systems which use high frequency Doppler\nradars or depth cameras to uniquely identify in-air gestures, AirWare does not\nrequire any external sensors. In our analysis, we use openly available APIs to\ninterface with the Samsung Galaxy S5 audio and proximity sensors for data\ncollection. We find that AirWare is not reliable enough for a deployable\ninteraction system when trying to classify a gesture set of 21 gestures, with\nan average true positive rate of only 50.5% per gesture. To improve\nperformance, we train AirWare to identify subsets of the 21 gestures vocabulary\nbased on possible usage scenarios. We find that AirWare can identify three\ngesture sets with average true positive rate greater than 80% using 4--7\ngestures per set, which comprises a vocabulary of 16 unique in-air gestures.",
      "categories": "cs.HC",
      "update_date": "2021-01-26",
      "doi": null,
      "authors": "Nibhrat Lohia, Raunak Mundada, Arya D. McCarthy, Eric C. Larson",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.709906893378087,
      "community_id": null
    },
    {
      "id": "962868",
      "arxiv_id": "1804.01174",
      "title": "Towards Deep Learning based Hand Keypoints Detection for Rapid\n  Sequential Movements from RGB Images",
      "abstract": "Hand keypoints detection and pose estimation has numerous applications in\ncomputer vision, but it is still an unsolved problem in many aspects. An\napplication of hand keypoints detection is in performing cognitive assessments\nof a subject by observing the performance of that subject in physical tasks\ninvolving rapid finger motion. As a part of this work, we introduce a novel\nhand key-points benchmark dataset that consists of hand gestures recorded\nspecifically for cognitive behavior monitoring. We explore the state of the art\nmethods in hand keypoint detection and we provide quantitative evaluations for\nthe performance of these methods on our dataset. In future, these results and\nour dataset can serve as a useful benchmark for hand keypoint recognition for\nrapid finger movements.",
      "categories": "cs.CV",
      "update_date": "2018-04-05",
      "doi": null,
      "authors": "Srujana Gattupalli, Ashwin Ramesh Babu, James Robert Brady, Fillia\n  Makedon, Vassilis Athitsos",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.709889923347244,
      "community_id": null
    },
    {
      "id": "1610287",
      "arxiv_id": "2202.11462",
      "title": "Thermal hand image segmentation for biometric recognition",
      "abstract": "In this paper we present a method to identify people by means of thermal (TH)\nand visible (VIS) hand images acquired simultaneously with a TESTO 882-3\ncamera. In addition, we also present a new database specially acquired for this\nwork. The real challenge when dealing with TH images is the cold finger areas,\nwhich can be confused with the acquisition surface. This problem is solved by\ntaking advantage of the VIS information. We have performed different tests to\nshow how TH and VIS images work in identification problems. Experimental\nresults reveal that TH hand image is as suitable for biometric recognition\nsystems as VIS hand images, and better results are obtained when combining this\ninformation. A Biometric Dispersion Matcher has been used as a feature vector\ndimensionality reduction technique as well as a classification task. Its\nselection criteria helps to reduce the length of the vectors used to perform\nidentification up to a hundred measurements. Identification rates reach a\nmaximum value of 98.3% under these conditions, when using a database of 104\npeople.",
      "categories": "cs.CV cs.CR cs.LG",
      "update_date": "2022-02-24",
      "doi": "10.1109/MAES.2013.6533739",
      "authors": "Xavier Font-Aragones, Marcos Faundez-Zanuy, Jiri Mekyska",
      "comments": "12 pages",
      "journal_ref": "IEEE Aerospace and Electronic Systems Magazine, vol. 28, no. 6,\n  pp. 4-14, June 2013",
      "citation_count": null,
      "relevance": 0.709884720451552,
      "community_id": null
    },
    {
      "id": "437384",
      "arxiv_id": "1306.2599",
      "title": "Hand Gesture Recognition Based on Karhunen-Loeve Transform",
      "abstract": "In this paper, we have proposed a system based on K-L Transform to recognize\ndifferent hand gestures. The system consists of five steps: skin filtering,\npalm cropping, edge detection, feature extraction, and classification. Firstly\nthe hand is detected using skin filtering and palm cropping was performed to\nextract out only the palm portion of the hand. The extracted image was then\nprocessed using the Canny Edge Detection technique to extract the outline\nimages of palm. After palm extraction, the features of hand were extracted\nusing K-L Transform technique and finally the input gesture was recognized\nusing proper classifier. In our system, we have tested for 10 different hand\ngestures, and recognizing rate obtained was 96%. Hence we propose an easy\napproach to recognize different hand gestures.",
      "categories": "cs.CV",
      "update_date": "2013-06-12",
      "doi": null,
      "authors": "Joyeeta Singha and Karen Das",
      "comments": "7 pages,9 figures",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.709779714372192,
      "community_id": null
    },
    {
      "id": "1229577",
      "arxiv_id": "2001.04163",
      "title": "Towards Interpretable and Robust Hand Detection via Pixel-wise\n  Prediction",
      "abstract": "The lack of interpretability of existing CNN-based hand detection methods\nmakes it difficult to understand the rationale behind their predictions. In\nthis paper, we propose a novel neural network model, which introduces\ninterpretability into hand detection for the first time. The main improvements\ninclude: (1) Detect hands at pixel level to explain what pixels are the basis\nfor its decision and improve transparency of the model. (2) The explainable\nHighlight Feature Fusion block highlights distinctive features among multiple\nlayers and learns discriminative ones to gain robust performance. (3) We\nintroduce a transparent representation, the rotation map, to learn rotation\nfeatures instead of complex and non-transparent rotation and derotation layers.\n(4) Auxiliary supervision accelerates the training process, which saves more\nthan 10 hours in our experiments. Experimental results on the VIVA and Oxford\nhand detection and tracking datasets show competitive accuracy of our method\ncompared with state-of-the-art methods with higher speed.",
      "categories": "cs.CV",
      "update_date": "2020-01-20",
      "doi": "10.1016/j.patcog.2020.107202",
      "authors": "Dan Liu, Libo Zhang, Tiejian Luo, Lili Tao, Yanjun Wu",
      "comments": "Accepted to Pattern Recognition",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.709562395863247,
      "community_id": null
    },
    {
      "id": "260559",
      "arxiv_id": "1105.1293",
      "title": "Eigengestures for natural human computer interface",
      "abstract": "We present the application of Principal Component Analysis for data acquired\nduring the design of a natural gesture interface. We investigate the concept of\nan eigengesture for motion capture hand gesture data and present the\nvisualisation of principal components obtained in the course of conducted\nexperiments. We also show the influence of dimensionality reduction on\nreconstructed gesture data quality.",
      "categories": "cs.HC",
      "update_date": "2011-09-07",
      "doi": "10.1007/978-3-642-23169-8_6",
      "authors": "Piotr Gawron, Przemys{\\l}aw G{\\l}omb, Jaros{\\l}aw Adam Miszczak,\n  Zbigniew Pucha{\\l}a",
      "comments": "10 pages, 3 figures",
      "journal_ref": "Advances in Intelligent and Soft Computing, 2011, Volume 103/2011,\n  49-56",
      "citation_count": null,
      "relevance": 0.709185915637857,
      "community_id": null
    },
    {
      "id": "1259109",
      "arxiv_id": "2003.08753",
      "title": "FineHand: Learning Hand Shapes for American Sign Language Recognition",
      "abstract": "American Sign Language recognition is a difficult gesture recognition\nproblem, characterized by fast, highly articulate gestures. These are comprised\nof arm movements with different hand shapes, facial expression and head\nmovements. Among these components, hand shape is the vital, often the most\ndiscriminative part of a gesture. In this work, we present an approach for\neffective learning of hand shape embeddings, which are discriminative for ASL\ngestures. For hand shape recognition our method uses a mix of manually labelled\nhand shapes and high confidence predictions to train deep convolutional neural\nnetwork (CNN). The sequential gesture component is captured by recursive neural\nnetwork (RNN) trained on the embeddings learned in the first stage. We will\ndemonstrate that higher quality hand shape models can significantly improve the\naccuracy of final video gesture classification in challenging conditions with\nvariety of speakers, different illumination and significant motion blurr. We\ncompare our model to alternative approaches exploiting different modalities and\nrepresentations of the data and show improved video gesture recognition\naccuracy on GMU-ASL51 benchmark dataset",
      "categories": "cs.CV cs.HC cs.LG stat.ML",
      "update_date": "2020-03-20",
      "doi": null,
      "authors": "Al Amin Hosain, Panneer Selvam Santhalingam, Parth Pathak, Huzefa\n  Rangwala and Jana Kosecka",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.709177153755597,
      "community_id": null
    },
    {
      "id": "1173573",
      "arxiv_id": "1909.03534",
      "title": "A New GNG Graph-Based Hand Gesture Recognition Approach",
      "abstract": "Hand Gesture Recognition (HGR) is of major importance for Human-Computer\nInteraction (HCI) applications. In this paper, we present a new hand gesture\nrecognition approach called GNG-IEMD. In this approach, first, we use a Growing\nNeural Gas (GNG) graph to model the image. Then we extract features from this\ngraph. These features are not geometric or pixel-based, so do not depend on\nscale, rotation, and articulation. The dissimilarity between hand gestures is\nmeasured with a novel Improved Earth Mover\\textquotesingle s Distance (IEMD)\nmetric. We evaluate the performance of the proposed approach on challenging\npublic datasets including NTU Hand Digits, HKU, HKU multi-angle, and UESTC-ASL\nand compare the results with state-of-the-art approaches. The experimental\nresults demonstrate the performance of the proposed approach.",
      "categories": "cs.CV",
      "update_date": "2019-09-10",
      "doi": null,
      "authors": "Narges Mirehi and Maryam Tahmasbi",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.709141408307385,
      "community_id": null
    },
    {
      "id": "1496556",
      "arxiv_id": "2107.02543",
      "title": "A Deep Learning-based Multimodal Depth-Aware Dynamic Hand Gesture\n  Recognition System",
      "abstract": "The dynamic hand gesture recognition task has seen studies on various\nunimodal and multimodal methods. Previously, researchers have explored depth\nand 2D-skeleton-based multimodal fusion CRNNs (Convolutional Recurrent Neural\nNetworks) but have had limitations in getting expected recognition results. In\nthis paper, we revisit this approach to hand gesture recognition and suggest\nseveral improvements. We observe that raw depth images possess low contrast in\nthe hand regions of interest (ROI). They do not highlight important fine\ndetails, such as finger orientation, overlap between the finger and palm, or\noverlap between multiple fingers. We thus propose quantizing the depth values\ninto several discrete regions, to create a higher contrast between several key\nparts of the hand. In addition, we suggest several ways to tackle the high\nvariance problem in existing multimodal fusion CRNN architectures. We evaluate\nour method on two benchmarks: the DHG-14/28 dataset and the SHREC'17 track\ndataset. Our approach shows a significant improvement in accuracy and parameter\nefficiency over previous similar multimodal methods, with a comparable result\nto the state-of-the-art.",
      "categories": "cs.CV cs.HC cs.LG",
      "update_date": "2021-11-08",
      "doi": null,
      "authors": "Hasan Mahmud, Mashrur M. Morshed, Md. Kamrul Hasan",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.708882766432599,
      "community_id": "20241021_QPMJ"
    },
    {
      "id": "1222270",
      "arxiv_id": "1912.10373",
      "title": "Robust Pose Invariant Shape and Texture based Hand Recognition",
      "abstract": "This paper presents a novel personal identification and verification system\nusing information extracted from the hand shape and texture. The system has two\nmajor constituent modules: a fully automatic and robust peg free segmentation\nand pose normalisation module, and a recognition module. In the first module,\nthe hand is segmented from its background using a thresholding technique based\non Otsu`s method combined with a skin colour detector. A set of fully automatic\nalgorithms are then proposed to segment the palm and fingers. In these\nalgorithms, the skeleton and the contour of the hand and fingers are estimated\nand used to determine the global pose of the hand and the pose of each\nindividual finger. Finally the palm and fingers are cropped, pose corrected and\nnormalised. In the recognition module, various shape and texture based features\nare extracted and used for matching purposes. The modified Hausdorff distance,\nthe Iterative Closest Point (ICP) and Independent Component Analysis (ICA)\nalgorithms are used for shape and texture features of the fingers. For the\npalmprints, we use the Discrete Cosine Transform (DCT), directional line\nfeatures and ICA. Recognition (identification and verification) tests were\nperformed using fusion strategies based on the similarity scores of the fingers\nand the palm. Experimental results show that the proposed system exhibits a\nsuperior performance over existing systems with an accuracy of over 98\\% for\nhand identification and verification (at equal error rate) in a database of 560\ndifferent subjects.",
      "categories": "cs.CV",
      "update_date": "2019-12-24",
      "doi": null,
      "authors": "F. Sohel, A. El-Sallam, and M. Bennamoun",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.708780902100749,
      "community_id": null
    },
    {
      "id": "541656",
      "arxiv_id": "1407.4898",
      "title": "Hand Pointing Detection Using Live Histogram Template of Forehead Skin",
      "abstract": "Hand pointing detection has multiple applications in many fields such as\nvirtual reality and control devices in smart homes. In this paper, we proposed\na novel approach to detect pointing vector in 2D space of a room. After\nbackground subtraction, face and forehead is detected. In the second step,\nforehead skin H-S plane histograms in HSV space is calculated. By using these\nhistogram templates of users skin, and back projection method, skin areas are\ndetected. The contours of hand are extracted using Freeman chain code\nalgorithm. Next step is finding fingertips. Points in hand contour which are\ncandidates for the fingertip can be found in convex defects of convex hull and\ncontour. We introduced a novel method for finding the fingertip based on the\nspecial points on the contour and their relationships. Our approach detects\nhand-pointing vectors in live video from a common webcam with 94%TP and 85%TN.",
      "categories": "cs.CV",
      "update_date": "2014-07-21",
      "doi": null,
      "authors": "Ghassem Tofighi, Nasser Ali Afarin, Kamraan Raahemifar, Anastasios N.\n  Venetsanopoulos",
      "comments": "Accepted for oral presentation in DSP2014",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.708196938037877,
      "community_id": null
    },
    {
      "id": "1732740",
      "arxiv_id": "2210.11384",
      "title": "Transformer-based Global 3D Hand Pose Estimation in Two Hands\n  Manipulating Objects Scenarios",
      "abstract": "This report describes our 1st place solution to ECCV 2022 challenge on Human\nBody, Hands, and Activities (HBHA) from Egocentric and Multi-view Cameras (hand\npose estimation). In this challenge, we aim to estimate global 3D hand poses\nfrom the input image where two hands and an object are interacting on the\negocentric viewpoint. Our proposed method performs end-to-end multi-hand pose\nestimation via transformer architecture. In particular, our method robustly\nestimates hand poses in a scenario where two hands interact. Additionally, we\npropose an algorithm that considers hand scales to robustly estimate the\nabsolute depth. The proposed algorithm works well even when the hand sizes are\nvarious for each person. Our method attains 14.4 mm (left) and 15.9 mm (right)\nerrors for each hand in the test set.",
      "categories": "cs.CV",
      "update_date": "2022-10-21",
      "doi": null,
      "authors": "Hoseong Cho, Donguk Kim, Chanwoo Kim, Seongyeong Lee and Seungryul\n  Baek",
      "comments": "5 pages",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.708102503177452,
      "community_id": "20241021_1KS44"
    },
    {
      "id": "618284",
      "arxiv_id": "1504.06378",
      "title": "Depth-based hand pose estimation: methods, data, and challenges",
      "abstract": "Hand pose estimation has matured rapidly in recent years. The introduction of\ncommodity depth sensors and a multitude of practical applications have spurred\nnew advances. We provide an extensive analysis of the state-of-the-art,\nfocusing on hand pose estimation from a single depth frame. To do so, we have\nimplemented a considerable number of systems, and will release all software and\nevaluation code. We summarize important conclusions here: (1) Pose estimation\nappears roughly solved for scenes with isolated hands. However, methods still\nstruggle to analyze cluttered scenes where hands may be interacting with nearby\nobjects and surfaces. To spur further progress we introduce a challenging new\ndataset with diverse, cluttered scenes. (2) Many methods evaluate themselves\nwith disparate criteria, making comparisons difficult. We define a consistent\nevaluation criteria, rigorously motivated by human experiments. (3) We\nintroduce a simple nearest-neighbor baseline that outperforms most existing\nsystems. This implies that most systems do not generalize beyond their training\nsets. This also reinforces the under-appreciated point that training data is as\nimportant as the model itself. We conclude with directions for future progress.",
      "categories": "cs.CV",
      "update_date": "2015-05-08",
      "doi": null,
      "authors": "James Steven Supancic III, Gregory Rogez, Yi Yang, Jamie Shotton, Deva\n  Ramanan",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.708053146832806,
      "community_id": "20241021_HKFN"
    },
    {
      "id": "669837",
      "arxiv_id": "1510.05879",
      "title": "What's the point? Frame-wise Pointing Gesture Recognition with\n  Latent-Dynamic Conditional Random Fields",
      "abstract": "We use Latent-Dynamic Conditional Random Fields to perform skeleton-based\npointing gesture classification at each time instance of a video sequence,\nwhere we achieve a frame-wise pointing accuracy of roughly 83%. Subsequently,\nwe determine continuous time sequences of arbitrary length that form individual\npointing gestures and this way reliably detect pointing gestures at a false\npositive detection rate of 0.63%.",
      "categories": "cs.HC cs.CV cs.RO",
      "update_date": "2015-10-21",
      "doi": null,
      "authors": "Christian Wittner and Boris Schauerte and Rainer Stiefelhagen",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.7080121810429,
      "community_id": null
    },
    {
      "id": "1099030",
      "arxiv_id": "1903.06643",
      "title": "GestureKeeper: Gesture Recognition for Controlling Devices in IoT\n  Environments",
      "abstract": "This paper introduces and evaluates the GestureKeeper, a robust hand-gesture\nrecognition system based on a wearable inertial measurements unit (IMU). The\nidentification of the time windows where the gestures occur, without relying on\nan explicit user action or a special gesture marker, is a very challenging\ntask. To address this problem, GestureKeeper identifies the start of a gesture\nby exploiting the underlying dynamics of the associated time series using a\nrecurrence quantification analysis (RQA). RQA is a powerful method for\nnonlinear time-series analysis, which enables the detection of critical\ntransitions in the system's dynamical behavior. Most importantly, it does not\nmake any assumption about the underlying distribution or model that governs the\ndata. Having estimated the gesture window, a support vector machine is employed\nto recognize the specific gesture. Our proposed method is evaluated by means of\na small-scale pilot study at FORTH and demonstrated that GestureKeeper can\nidentify correctly the start of a gesture with a 87\\% mean balanced accuracy\nand classify correctly the specific hand-gesture with a mean accuracy of over\n96\\%. To the best of our knowledge, GestureKeeper is the first automatic\nhand-gesture identification system based only on accelerometer. The performance\nanalysis reveals the predictive power of the features and the system's\nrobustness in the presence of additive noise. We also performed a sensitivity\nanalysis to examine the impact of various parameters and a comparative analysis\nof different classifiers (SVM, random forests). Most importantly, the system\ncan be extended to incorporate a large dictionary of gestures and operate\nwithout further calibration for a new user.",
      "categories": "cs.HC",
      "update_date": "2019-03-18",
      "doi": null,
      "authors": "Vasileios Sideridis, Andrew Zacharakis, George Tzagkarakis, Maria\n  Papadopouli",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.707634585640754,
      "community_id": null
    },
    {
      "id": "1156730",
      "arxiv_id": "1907.12193",
      "title": "ChaLearn Looking at People: IsoGD and ConGD Large-scale RGB-D Gesture\n  Recognition",
      "abstract": "The ChaLearn large-scale gesture recognition challenge has been run twice in\ntwo workshops in conjunction with the International Conference on Pattern\nRecognition (ICPR) 2016 and International Conference on Computer Vision (ICCV)\n2017, attracting more than $200$ teams round the world. This challenge has two\ntracks, focusing on isolated and continuous gesture recognition, respectively.\nThis paper describes the creation of both benchmark datasets and analyzes the\nadvances in large-scale gesture recognition based on these two datasets. We\ndiscuss the challenges of collecting large-scale ground-truth annotations of\ngesture recognition, and provide a detailed analysis of the current\nstate-of-the-art methods for large-scale isolated and continuous gesture\nrecognition based on RGB-D video sequences. In addition to recognition rate and\nmean jaccard index (MJI) as evaluation metrics used in our previous challenges,\nwe also introduce the corrected segmentation rate (CSR) metric to evaluate the\nperformance of temporal segmentation for continuous gesture recognition.\nFurthermore, we propose a bidirectional long short-term memory (Bi-LSTM)\nbaseline method, determining the video division points based on the skeleton\npoints extracted by convolutional pose machine (CPM). Experiments demonstrate\nthat the proposed Bi-LSTM outperforms the state-of-the-art methods with an\nabsolute improvement of $8.1\\%$ (from $0.8917$ to $0.9639$) of CSR.",
      "categories": "cs.CV",
      "update_date": "2020-07-30",
      "doi": "10.1109/TCYB.2020.3012092",
      "authors": "Jun Wan, Chi Lin, Longyin Wen, Yunan Li, Qiguang Miao, Sergio\n  Escalera, Gholamreza Anbarjafari, Isabelle Guyon, Guodong Guo, Stan Z. Li",
      "comments": "14 pages, 8 figures, 6 tables",
      "journal_ref": "IEEE Transactions on Cybernetics 2020",
      "citation_count": null,
      "relevance": 0.707473235876965,
      "community_id": null
    },
    {
      "id": "229453",
      "arxiv_id": "1012.0084",
      "title": "Survey on Various Gesture Recognition Techniques for Interfacing\n  Machines Based on Ambient Intelligence",
      "abstract": "Gesture recognition is mainly apprehensive on analyzing the functionality of\nhuman wits. The main goal of gesture recognition is to create a system which\ncan recognize specific human gestures and use them to convey information or for\ndevice control. Hand gestures provide a separate complementary modality to\nspeech for expressing ones ideas. Information associated with hand gestures in\na conversation is degree,discourse structure, spatial and temporal structure.\nThe approaches present can be mainly divided into Data-Glove Based and Vision\nBased approaches. An important face feature point is the nose tip. Since nose\nis the highest protruding point from the face. Besides that, it is not affected\nby facial expressions.Another important function of the nose is that it is able\nto indicate the head pose. Knowledge of the nose location will enable us to\nalign an unknown 3D face with those in a face database. Eye detection is\ndivided into eye position detection and eye contour detection. Existing works\nin eye detection can be classified into two major categories: traditional\nimage-based passive approaches and the active IR based approaches. The former\nuses intensity and shape of eyes for detection and the latter works on the\nassumption that eyes have a reflection under near IR illumination and produce\nbright/dark pupil effect. The traditional methods can be broadly classified\ninto three categories: template based methods,appearance based methods and\nfeature based methods. The purpose of this paper is to compare various human\nGesture recognition systems for interfacing machines directly to human wits\nwithout any corporeal media in an ambient environment.",
      "categories": "cs.AI cs.CV cs.HC cs.RO",
      "update_date": "2010-12-02",
      "doi": "10.5121/ijcses.2010.1203",
      "authors": "Harshith C, Karthik R. Shastry, Manoj Ravindran, M.V.V.N.S. Srikanth,\n  Naveen Lakshmikhanth",
      "comments": "12 PAGES",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.707322478294378,
      "community_id": null
    },
    {
      "id": "1092487",
      "arxiv_id": "1903.00100",
      "title": "Appearance-based Gesture recognition in the compressed domain",
      "abstract": "We propose a novel appearance-based gesture recognition algorithm using\ncompressed domain signal processing techniques. Gesture features are extracted\ndirectly from the compressed measurements, which are the block averages and the\ncoded linear combinations of the image sensor's pixel values. We also improve\nboth the computational efficiency and the memory requirement of the previous\nDTW-based K-NN gesture classifiers. Both simulation testing and hardware\nimplementation strongly support the proposed algorithm.",
      "categories": "cs.CV cs.LG stat.ML",
      "update_date": "2019-03-04",
      "doi": "10.1109/ICASSP.2017.7952451",
      "authors": "Shaojie Xu, Anvesha Amaravati, Justin Romberg, Arijit Raychowdhury",
      "comments": "arXiv admin note: text overlap with arXiv:1605.08313",
      "journal_ref": "2017 IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP), New Orleans, LA, 2017, pp. 1722-1726",
      "citation_count": null,
      "relevance": 0.707013820765755,
      "community_id": null
    },
    {
      "id": "1025928",
      "arxiv_id": "1809.05911",
      "title": "Robust and customized methods for real-time hand gesture recognition\n  under object-occlusion",
      "abstract": "Dynamic hand tracking and gesture recognition is a hard task since there are\nmany joints on the fingers and each joint owns many degrees of freedom.\nBesides, object occlusion is also a thorny issue in finger tracking and posture\nrecognition. Therefore, we propose a robust and customized system for realtime\nhand tracking and gesture recognition under occlusion environment. First, we\nmodel the angles between hand keypoints and encode their relative coordinate\nvectors, then we introduce GAN to generate raw discrete sequence dataset.\nSecondly we propose a time series forecasting method in the prediction of\ndefined hand keypoint location. Finally, we define a sliding window matching\nmethod to complete gesture recognition. We analyze 11 kinds of typical gestures\nand show how to perform gesture recognition with the proposed method. Our work\ncan reach state of the art results and contribute to build a framework to\nimplement customized gesture recognition task.",
      "categories": "cs.HC",
      "update_date": "2018-09-18",
      "doi": null,
      "authors": "Zhishuai Han, Xiaojuan Ban, Xiaokun Wang and Di wu",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.706442945043515,
      "community_id": null
    },
    {
      "id": "2068082",
      "arxiv_id": "2405.10868",
      "title": "Air Signing and Privacy-Preserving Signature Verification for Digital\n  Documents",
      "abstract": "This paper presents a novel approach to the digital signing of electronic\ndocuments through the use of a camera-based interaction system, single-finger\ntracking for sign recognition, and multi commands executing hand gestures. The\nproposed solution, referred to as \"Air Signature,\" involves writing the\nsignature in front of the camera, rather than relying on traditional methods\nsuch as mouse drawing or physically signing on paper and showing it to a web\ncamera. The goal is to develop a state-of-the-art method for detecting and\ntracking gestures and objects in real-time. The proposed methods include\napplying existing gesture recognition and object tracking systems, improving\naccuracy through smoothing and line drawing, and maintaining continuity during\nfast finger movements. An evaluation of the fingertip detection, sketching, and\noverall signing process is performed to assess the effectiveness of the\nproposed solution. The secondary objective of this research is to develop a\nmodel that can effectively recognize the unique signature of a user. This type\nof signature can be verified by neural cores that analyze the movement, speed,\nand stroke pixels of the signing in real time. The neural cores use machine\nlearning algorithms to match air signatures to the individual's stored\nsignatures, providing a secure and efficient method of verification. Our\nproposed System does not require sensors or any hardware other than the camera.",
      "categories": "cs.CV cs.HC",
      "update_date": "2024-05-20",
      "doi": null,
      "authors": "P. Sarveswarasarma, T. Sathulakjan, V. J. V. Godfrey, Thanuja D.\n  Ambegoda",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.70633371434607,
      "community_id": null
    },
    {
      "id": "2327865",
      "arxiv_id": "2505.17303",
      "title": "UAV Control with Vision-based Hand Gesture Recognition over Edge-Computing",
      "abstract": "Gesture recognition presents a promising avenue for interfacing with unmanned aerial vehicles (UAVs) due to its intuitive nature and potential for precise interaction. This research conducts a comprehensive comparative analysis of vision-based hand gesture detection methodologies tailored for UAV Control. The existing gesture recognition approaches involving cropping, zooming, and color-based segmentation, do not work well for this kind of applications in dynamic conditions and suffer in performance with increasing distance and environmental noises. We propose to use a novel approach leveraging hand landmarks drawing and classification for gesture recognition based UAV control. With experimental results we show that our proposed method outperforms the other existing methods in terms of accuracy, noise resilience, and efficacy across varying distances, thus providing robust control decisions. However, implementing the deep learning based compute intensive gesture recognition algorithms on the UAV's onboard computer is significantly challenging in terms of performance. Hence, we propose to use a edge-computing based framework to offload the heavier computing tasks, thus achieving closed-loop real-time performance. With implementation over AirSim simulator as well as over a real-world UAV, we showcase the advantage of our end-to-end gesture recognition based UAV control system.",
      "categories": "cs.RO cs.SY eess.SY",
      "update_date": "2025-05-26",
      "doi": null,
      "authors": "Sousannah Abdalla and Sabur Baidya",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.706319528441459,
      "community_id": null
    },
    {
      "id": "469826",
      "arxiv_id": "1310.4822",
      "title": "Principal motion components for gesture recognition using a\n  single-example",
      "abstract": "This paper introduces principal motion components (PMC), a new method for\none-shot gesture recognition. In the considered scenario a single\ntraining-video is available for each gesture to be recognized, which limits the\napplication of traditional techniques (e.g., HMMs). In PMC, a 2D map of motion\nenergy is obtained per each pair of consecutive frames in a video. Motion maps\nassociated to a video are processed to obtain a PCA model, which is used for\nrecognition under a reconstruction-error approach. The main benefits of the\nproposed approach are its simplicity, easiness of implementation, competitive\nperformance and efficiency. We report experimental results in one-shot gesture\nrecognition using the ChaLearn Gesture Dataset; a benchmark comprising more\nthan 50,000 gestures, recorded as both RGB and depth video with a Kinect\ncamera. Results obtained with PMC are competitive with alternative methods\nproposed for the same data set.",
      "categories": "cs.CV",
      "update_date": "2014-02-03",
      "doi": null,
      "authors": "Hugo Jair Escalante, Isabelle Guyon, Vassilis Athitsos, Pat\n  Jangyodsuk, Jun Wan",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.706111648496641,
      "community_id": null
    },
    {
      "id": "1276394",
      "arxiv_id": "2004.11623",
      "title": "Low-latency hand gesture recognition with a low resolution thermal\n  imager",
      "abstract": "Using hand gestures to answer a call or to control the radio while driving a\ncar, is nowadays an established feature in more expensive cars. High resolution\ntime-of-flight cameras and powerful embedded processors usually form the heart\nof these gesture recognition systems. This however comes with a price tag. We\ntherefore investigate the possibility to design an algorithm that predicts hand\ngestures using a cheap low-resolution thermal camera with only 32x24 pixels,\nwhich is light-weight enough to run on a low-cost processor. We recorded a new\ndataset of over 1300 video clips for training and evaluation and propose a\nlight-weight low-latency prediction algorithm. Our best model achieves 95.9%\nclassification accuracy and 83% mAP detection accuracy while its processing\npipeline has a latency of only one frame.",
      "categories": "cs.CV",
      "update_date": "2020-04-27",
      "doi": null,
      "authors": "Maarten Vandersteegen, Wouter Reusen, Kristof Van Beeck Toon Goedeme",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.706004731773485,
      "community_id": null
    },
    {
      "id": "693186",
      "arxiv_id": "1601.01157",
      "title": "A simple technique for improving multi-class classification with neural\n  networks",
      "abstract": "We present a novel method to perform multi-class pattern classification with\nneural networks and test it on a challenging 3D hand gesture recognition\nproblem. Our method consists of a standard one-against-all (OAA)\nclassification, followed by another network layer classifying the resulting\nclass scores, possibly augmented by the original raw input vector. This allows\nthe network to disambiguate hard-to-separate classes as the distribution of\nclass scores carries considerable information as well, and is in fact often\nused for assessing the confidence of a decision. We show that by this approach\nwe are able to significantly boost our results, overall as well as for\nparticular difficult cases, on the hard 10-class gesture classification task.",
      "categories": "cs.LG",
      "update_date": "2016-01-07",
      "doi": null,
      "authors": "Thomas Kopinski, Alexander Gepperth (ENSTA ParisTech U2IS/RV,\n  Flowers), Uwe Handmann",
      "comments": "European Symposium on artificial neural networks (ESANN), Jun 2015,\n  Bruges, Belgium",
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.705785253471578,
      "community_id": null
    },
    {
      "id": "2060759",
      "arxiv_id": "2405.03545",
      "title": "Optimizing Hand Region Detection in MediaPipe Holistic Full-Body Pose\n  Estimation to Improve Accuracy and Avoid Downstream Errors",
      "abstract": "This paper addresses a critical flaw in MediaPipe Holistic's hand Region of\nInterest (ROI) prediction, which struggles with non-ideal hand orientations,\naffecting sign language recognition accuracy. We propose a data-driven approach\nto enhance ROI estimation, leveraging an enriched feature set including\nadditional hand keypoints and the z-dimension. Our results demonstrate better\nestimates, with higher Intersection-over-Union compared to the current method.\nOur code and optimizations are available at\nhttps://github.com/sign-language-processing/mediapipe-hand-crop-fix.",
      "categories": "cs.CV",
      "update_date": "2024-05-14",
      "doi": null,
      "authors": "Amit Moryossef",
      "comments": null,
      "journal_ref": null,
      "citation_count": null,
      "relevance": 0.705569302371824,
      "community_id": null
    },
    {
      "id": "1552605",
      "arxiv_id": "2110.14461",
      "title": "Hand gesture detection in tests performed by older adults",
      "abstract": "Our team are developing a new online test that analyses hand movement\nfeatures associated with ageing that can be completed remotely from the\nresearch centre. To obtain hand movement features, participants will be asked\nto perform a variety of hand gestures using their own computer cameras.\nHowever, it is challenging to collect high quality hand movement video data,\nespecially for older participants, many of whom have no IT background. During\nthe data collection process, one of the key steps is to detect whether the\nparticipants are following the test instructions correctly and also to detect\nsimilar gestures from different devices. Furthermore, we need this process to\nbe automated and accurate as we expect many thousands of participants to\ncomplete the test. We have implemented a hand gesture detector to detect the\ngestures in the hand movement tests and our detection mAP is 0.782 which is\nbetter than the state-of-the-art. In this research, we have processed 20,000\nimages collected from hand movement tests and labelled 6,450 images to detect\ndifferent hand gestures in the hand movement tests. This paper has the\nfollowing three contributions. Firstly, we compared and analysed the\nperformance of different network structures for hand gesture detection.\nSecondly, we have made many attempts to improve the accuracy of the model and\nhave succeeded in improving the classification accuracy for similar gestures by\nimplementing attention layers. Thirdly, we have created two datasets and\nincluded 20 percent of blurred images in the dataset to investigate how\ndifferent network structures were impacted by noisy data, our experiments have\nalso shown our network has better performance on the noisy dataset.",
      "categories": "cs.CV cs.AI",
      "update_date": "2023-01-26",
      "doi": "10.1007/s00521-022-08090-8",
      "authors": "Guan Huang and Son N. Tran and Quan Bai and Jane Alty",
      "comments": null,
      "journal_ref": "Neural Comput & Applic (2022)",
      "citation_count": null,
      "relevance": 0.705383413067447,
      "community_id": null
    }
  ],
  "links": [
    {
      "source": "2109291",
      "target": "1377609",
      "weight": 0.841534435749054
    },
    {
      "source": "2109291",
      "target": "289403",
      "weight": 0.834126949310303
    },
    {
      "source": "2109291",
      "target": "1611344",
      "weight": 0.833743035793304
    },
    {
      "source": "2109291",
      "target": "2364034",
      "weight": 0.83371365070343
    },
    {
      "source": "2109291",
      "target": "1401067",
      "weight": 0.828371286392212
    },
    {
      "source": "2235578",
      "target": "2364034",
      "weight": 0.911664545536041
    },
    {
      "source": "2235578",
      "target": "1962201",
      "weight": 0.87971967458725
    },
    {
      "source": "2235578",
      "target": "2125619",
      "weight": 0.877862215042114
    },
    {
      "source": "2235578",
      "target": "1536701",
      "weight": 0.876138627529144
    },
    {
      "source": "2235578",
      "target": "618284",
      "weight": 0.875064969062805
    },
    {
      "source": "1937177",
      "target": "1554142",
      "weight": 0.868972659111023
    },
    {
      "source": "1937177",
      "target": "1915663",
      "weight": 0.860073566436768
    },
    {
      "source": "1937177",
      "target": "485544",
      "weight": 0.855176568031311
    },
    {
      "source": "1937177",
      "target": "2235578",
      "weight": 0.848005950450897
    },
    {
      "source": "1937177",
      "target": "2203888",
      "weight": 0.843491017818451
    },
    {
      "source": "1407666",
      "target": "2235578",
      "weight": 0.857535481452942
    },
    {
      "source": "1407666",
      "target": "1536701",
      "weight": 0.852360010147095
    },
    {
      "source": "1407666",
      "target": "2007304",
      "weight": 0.850668907165527
    },
    {
      "source": "1407666",
      "target": "2125619",
      "weight": 0.839845240116119
    },
    {
      "source": "1407666",
      "target": "289403",
      "weight": 0.835687458515167
    },
    {
      "source": "2364034",
      "target": "2235578",
      "weight": 0.911664545536041
    },
    {
      "source": "2364034",
      "target": "2125619",
      "weight": 0.888116121292114
    },
    {
      "source": "2364034",
      "target": "1496556",
      "weight": 0.881965041160584
    },
    {
      "source": "2364034",
      "target": "1281930",
      "weight": 0.873745024204254
    },
    {
      "source": "2364034",
      "target": "1156730",
      "weight": 0.86937689781189
    },
    {
      "source": "1401067",
      "target": "841809",
      "weight": 0.865917801856995
    },
    {
      "source": "1401067",
      "target": "1281930",
      "weight": 0.862263083457947
    },
    {
      "source": "1401067",
      "target": "962868",
      "weight": 0.858993589878082
    },
    {
      "source": "1401067",
      "target": "1679011",
      "weight": 0.852681457996368
    },
    {
      "source": "1401067",
      "target": "887647",
      "weight": 0.849804043769836
    },
    {
      "source": "289403",
      "target": "260559",
      "weight": 0.843219101428986
    },
    {
      "source": "289403",
      "target": "1407666",
      "weight": 0.835687458515167
    },
    {
      "source": "289403",
      "target": "2109291",
      "weight": 0.834126949310303
    },
    {
      "source": "289403",
      "target": "997049",
      "weight": 0.825460135936737
    },
    {
      "source": "289403",
      "target": "2125619",
      "weight": 0.819467425346375
    },
    {
      "source": "1377609",
      "target": "841809",
      "weight": 0.861305236816406
    },
    {
      "source": "1377609",
      "target": "1679011",
      "weight": 0.849663138389587
    },
    {
      "source": "1377609",
      "target": "2109291",
      "weight": 0.841534435749054
    },
    {
      "source": "1377609",
      "target": "1079871",
      "weight": 0.84135639667511
    },
    {
      "source": "1377609",
      "target": "1251806",
      "weight": 0.838153004646301
    },
    {
      "source": "1802119",
      "target": "1801345",
      "weight": 0.861507058143616
    },
    {
      "source": "1802119",
      "target": "1276394",
      "weight": 0.834778964519501
    },
    {
      "source": "1802119",
      "target": "1610287",
      "weight": 0.829521775245667
    },
    {
      "source": "1802119",
      "target": "2235578",
      "weight": 0.826867818832398
    },
    {
      "source": "1802119",
      "target": "1554142",
      "weight": 0.826394855976105
    },
    {
      "source": "547066",
      "target": "1962201",
      "weight": 0.856281459331513
    },
    {
      "source": "547066",
      "target": "546856",
      "weight": 0.848618984222412
    },
    {
      "source": "547066",
      "target": "1377609",
      "weight": 0.823095321655273
    },
    {
      "source": "547066",
      "target": "2235578",
      "weight": 0.814517199993134
    },
    {
      "source": "547066",
      "target": "131591",
      "weight": 0.814430415630341
    },
    {
      "source": "2007304",
      "target": "1407666",
      "weight": 0.850668907165527
    },
    {
      "source": "2007304",
      "target": "131591",
      "weight": 0.833230376243591
    },
    {
      "source": "2007304",
      "target": "1222270",
      "weight": 0.827034652233124
    },
    {
      "source": "2007304",
      "target": "2109291",
      "weight": 0.819654107093811
    },
    {
      "source": "2007304",
      "target": "1802119",
      "weight": 0.812007784843445
    },
    {
      "source": "1074170",
      "target": "2235578",
      "weight": 0.849791169166565
    },
    {
      "source": "1074170",
      "target": "2364034",
      "weight": 0.833664774894714
    },
    {
      "source": "1074170",
      "target": "485544",
      "weight": 0.832956790924072
    },
    {
      "source": "1074170",
      "target": "1496556",
      "weight": 0.831104338169098
    },
    {
      "source": "1074170",
      "target": "1320362",
      "weight": 0.830329298973084
    },
    {
      "source": "1747974",
      "target": "1537556",
      "weight": 0.961905956268311
    },
    {
      "source": "1747974",
      "target": "887647",
      "weight": 0.880366027355194
    },
    {
      "source": "1747974",
      "target": "618284",
      "weight": 0.862497389316559
    },
    {
      "source": "1747974",
      "target": "753816",
      "weight": 0.84842973947525
    },
    {
      "source": "1747974",
      "target": "1732740",
      "weight": 0.84403121471405
    },
    {
      "source": "2006061",
      "target": "1915663",
      "weight": 0.88279139995575
    },
    {
      "source": "2006061",
      "target": "997049",
      "weight": 0.852405786514282
    },
    {
      "source": "2006061",
      "target": "2235578",
      "weight": 0.848502814769745
    },
    {
      "source": "2006061",
      "target": "1824366",
      "weight": 0.831544518470764
    },
    {
      "source": "2006061",
      "target": "1281930",
      "weight": 0.828186988830566
    },
    {
      "source": "1554142",
      "target": "1915663",
      "weight": 0.901887536048889
    },
    {
      "source": "1554142",
      "target": "1937177",
      "weight": 0.868972659111023
    },
    {
      "source": "1554142",
      "target": "2125619",
      "weight": 0.867332935333252
    },
    {
      "source": "1554142",
      "target": "997049",
      "weight": 0.863463282585144
    },
    {
      "source": "1554142",
      "target": "1079871",
      "weight": 0.859073162078857
    },
    {
      "source": "1056631",
      "target": "1962201",
      "weight": 0.808405160903931
    },
    {
      "source": "1056631",
      "target": "547066",
      "weight": 0.801322340965271
    },
    {
      "source": "1056631",
      "target": "1519786",
      "weight": 0.771291732788086
    },
    {
      "source": "1056631",
      "target": "1229101",
      "weight": 0.770447373390198
    },
    {
      "source": "1056631",
      "target": "885594",
      "weight": 0.76955634355545
    },
    {
      "source": "2227588",
      "target": "1113546",
      "weight": 0.794779896736145
    },
    {
      "source": "2227588",
      "target": "841809",
      "weight": 0.793997168540955
    },
    {
      "source": "2227588",
      "target": "1144358",
      "weight": 0.793533027172089
    },
    {
      "source": "2227588",
      "target": "1937177",
      "weight": 0.792681634426117
    },
    {
      "source": "2227588",
      "target": "1229101",
      "weight": 0.787748277187347
    },
    {
      "source": "1537556",
      "target": "1747974",
      "weight": 0.961905956268311
    },
    {
      "source": "1537556",
      "target": "887647",
      "weight": 0.888173341751099
    },
    {
      "source": "1537556",
      "target": "618284",
      "weight": 0.866984486579895
    },
    {
      "source": "1537556",
      "target": "798699",
      "weight": 0.856101155281067
    },
    {
      "source": "1537556",
      "target": "1233461",
      "weight": 0.849717020988464
    },
    {
      "source": "1079871",
      "target": "1281930",
      "weight": 0.871910095214844
    },
    {
      "source": "1079871",
      "target": "1496556",
      "weight": 0.868310391902924
    },
    {
      "source": "1079871",
      "target": "2364034",
      "weight": 0.866532325744629
    },
    {
      "source": "1079871",
      "target": "1176711",
      "weight": 0.864080131053925
    },
    {
      "source": "1079871",
      "target": "1554142",
      "weight": 0.859073162078857
    },
    {
      "source": "642905",
      "target": "1937177",
      "weight": 0.808481991291046
    },
    {
      "source": "642905",
      "target": "1962201",
      "weight": 0.80727481842041
    },
    {
      "source": "642905",
      "target": "1915663",
      "weight": 0.797064602375031
    },
    {
      "source": "642905",
      "target": "745155",
      "weight": 0.795583486557007
    },
    {
      "source": "642905",
      "target": "131591",
      "weight": 0.795362234115601
    },
    {
      "source": "960478",
      "target": "2327865",
      "weight": 0.880781710147858
    },
    {
      "source": "960478",
      "target": "2235578",
      "weight": 0.814121544361115
    },
    {
      "source": "960478",
      "target": "1401067",
      "weight": 0.807543396949768
    },
    {
      "source": "960478",
      "target": "841809",
      "weight": 0.794991314411163
    },
    {
      "source": "960478",
      "target": "2364034",
      "weight": 0.791438817977905
    },
    {
      "source": "1083721",
      "target": "1229101",
      "weight": 0.8690105676651
    },
    {
      "source": "1083721",
      "target": "2235578",
      "weight": 0.850182175636292
    },
    {
      "source": "1083721",
      "target": "1536701",
      "weight": 0.839909493923187
    },
    {
      "source": "1083721",
      "target": "1401067",
      "weight": 0.836566269397736
    },
    {
      "source": "1083721",
      "target": "618284",
      "weight": 0.834275662899017
    },
    {
      "source": "745155",
      "target": "841809",
      "weight": 0.849633812904358
    },
    {
      "source": "745155",
      "target": "1229101",
      "weight": 0.819790422916412
    },
    {
      "source": "745155",
      "target": "1401067",
      "weight": 0.808419585227966
    },
    {
      "source": "745155",
      "target": "1962201",
      "weight": 0.807968974113464
    },
    {
      "source": "745155",
      "target": "1937177",
      "weight": 0.807100415229797
    },
    {
      "source": "841809",
      "target": "1679011",
      "weight": 0.901769697666168
    },
    {
      "source": "841809",
      "target": "1401067",
      "weight": 0.865917801856995
    },
    {
      "source": "841809",
      "target": "1377609",
      "weight": 0.861305236816406
    },
    {
      "source": "841809",
      "target": "745155",
      "weight": 0.849633812904358
    },
    {
      "source": "841809",
      "target": "1079871",
      "weight": 0.835371017456055
    },
    {
      "source": "1381649",
      "target": "887647",
      "weight": 0.876830041408539
    },
    {
      "source": "1381649",
      "target": "753816",
      "weight": 0.859608888626099
    },
    {
      "source": "1381649",
      "target": "798699",
      "weight": 0.850196421146393
    },
    {
      "source": "1381649",
      "target": "1537556",
      "weight": 0.847728967666626
    },
    {
      "source": "1381649",
      "target": "1747974",
      "weight": 0.841832458972931
    },
    {
      "source": "485544",
      "target": "1937177",
      "weight": 0.855176568031311
    },
    {
      "source": "485544",
      "target": "1156730",
      "weight": 0.854965746402741
    },
    {
      "source": "485544",
      "target": "997049",
      "weight": 0.84995973110199
    },
    {
      "source": "485544",
      "target": "1144358",
      "weight": 0.847323179244995
    },
    {
      "source": "485544",
      "target": "1554142",
      "weight": 0.841816544532776
    },
    {
      "source": "862266",
      "target": "485544",
      "weight": 0.836592733860016
    },
    {
      "source": "862266",
      "target": "1332360",
      "weight": 0.80944299697876
    },
    {
      "source": "862266",
      "target": "1407666",
      "weight": 0.80897068977356
    },
    {
      "source": "862266",
      "target": "2364034",
      "weight": 0.80738890171051
    },
    {
      "source": "862266",
      "target": "2235578",
      "weight": 0.804650247097015
    },
    {
      "source": "1915663",
      "target": "1554142",
      "weight": 0.901887536048889
    },
    {
      "source": "1915663",
      "target": "2006061",
      "weight": 0.88279139995575
    },
    {
      "source": "1915663",
      "target": "997049",
      "weight": 0.873544812202454
    },
    {
      "source": "1915663",
      "target": "1937177",
      "weight": 0.860073566436768
    },
    {
      "source": "1915663",
      "target": "2125619",
      "weight": 0.850586175918579
    },
    {
      "source": "1515353",
      "target": "554476",
      "weight": 0.800664305686951
    },
    {
      "source": "1515353",
      "target": "1407666",
      "weight": 0.790213346481323
    },
    {
      "source": "1515353",
      "target": "2007304",
      "weight": 0.776613473892212
    },
    {
      "source": "1515353",
      "target": "1401067",
      "weight": 0.775929987430573
    },
    {
      "source": "1515353",
      "target": "1042006",
      "weight": 0.774607241153717
    },
    {
      "source": "1320362",
      "target": "1496556",
      "weight": 0.850444912910461
    },
    {
      "source": "1320362",
      "target": "1079871",
      "weight": 0.840380549430847
    },
    {
      "source": "1320362",
      "target": "2364034",
      "weight": 0.836815357208252
    },
    {
      "source": "1320362",
      "target": "1113546",
      "weight": 0.834070801734924
    },
    {
      "source": "1320362",
      "target": "1401067",
      "weight": 0.831110894680023
    },
    {
      "source": "2203888",
      "target": "1668640",
      "weight": 0.953672766685486
    },
    {
      "source": "2203888",
      "target": "2125619",
      "weight": 0.855494320392609
    },
    {
      "source": "2203888",
      "target": "1915663",
      "weight": 0.844445168972015
    },
    {
      "source": "2203888",
      "target": "1937177",
      "weight": 0.843491017818451
    },
    {
      "source": "2203888",
      "target": "1554142",
      "weight": 0.842978000640869
    },
    {
      "source": "131591",
      "target": "1222270",
      "weight": 0.83450198173523
    },
    {
      "source": "131591",
      "target": "2007304",
      "weight": 0.833230376243591
    },
    {
      "source": "131591",
      "target": "2109291",
      "weight": 0.820382356643677
    },
    {
      "source": "131591",
      "target": "547066",
      "weight": 0.814430415630341
    },
    {
      "source": "131591",
      "target": "289403",
      "weight": 0.811761498451233
    },
    {
      "source": "1229101",
      "target": "1083721",
      "weight": 0.8690105676651
    },
    {
      "source": "1229101",
      "target": "1962201",
      "weight": 0.838064551353455
    },
    {
      "source": "1229101",
      "target": "2235578",
      "weight": 0.827883839607239
    },
    {
      "source": "1229101",
      "target": "745155",
      "weight": 0.819790422916412
    },
    {
      "source": "1229101",
      "target": "1536701",
      "weight": 0.809498190879822
    },
    {
      "source": "1536701",
      "target": "2235578",
      "weight": 0.876138627529144
    },
    {
      "source": "1536701",
      "target": "1093400",
      "weight": 0.86840957403183
    },
    {
      "source": "1536701",
      "target": "962868",
      "weight": 0.864267706871033
    },
    {
      "source": "1536701",
      "target": "618284",
      "weight": 0.858364939689636
    },
    {
      "source": "1536701",
      "target": "731505",
      "weight": 0.856016874313355
    },
    {
      "source": "1611344",
      "target": "1679011",
      "weight": 0.833911001682282
    },
    {
      "source": "1611344",
      "target": "2109291",
      "weight": 0.833743035793304
    },
    {
      "source": "1611344",
      "target": "2364034",
      "weight": 0.830236852169037
    },
    {
      "source": "1611344",
      "target": "1377609",
      "weight": 0.829698085784912
    },
    {
      "source": "1611344",
      "target": "1079871",
      "weight": 0.820706903934479
    },
    {
      "source": "1962201",
      "target": "2235578",
      "weight": 0.87971967458725
    },
    {
      "source": "1962201",
      "target": "2125619",
      "weight": 0.874062836170197
    },
    {
      "source": "1962201",
      "target": "547066",
      "weight": 0.856281459331513
    },
    {
      "source": "1962201",
      "target": "2364034",
      "weight": 0.845398783683777
    },
    {
      "source": "1962201",
      "target": "997049",
      "weight": 0.843757271766663
    },
    {
      "source": "1113546",
      "target": "1496556",
      "weight": 0.857646822929382
    },
    {
      "source": "1113546",
      "target": "1251806",
      "weight": 0.855312466621399
    },
    {
      "source": "1113546",
      "target": "1144358",
      "weight": 0.853137493133545
    },
    {
      "source": "1113546",
      "target": "1079871",
      "weight": 0.851432859897614
    },
    {
      "source": "1113546",
      "target": "2364034",
      "weight": 0.842779397964478
    },
    {
      "source": "2125619",
      "target": "2364034",
      "weight": 0.888116121292114
    },
    {
      "source": "2125619",
      "target": "2235578",
      "weight": 0.877862215042114
    },
    {
      "source": "2125619",
      "target": "1962201",
      "weight": 0.874062836170197
    },
    {
      "source": "2125619",
      "target": "1554142",
      "weight": 0.867332935333252
    },
    {
      "source": "2125619",
      "target": "1496556",
      "weight": 0.863284587860107
    },
    {
      "source": "1472623",
      "target": "131591",
      "weight": 0.78095817565918
    },
    {
      "source": "1472623",
      "target": "289403",
      "weight": 0.776761174201965
    },
    {
      "source": "1472623",
      "target": "296757",
      "weight": 0.773539662361145
    },
    {
      "source": "1472623",
      "target": "2007304",
      "weight": 0.773171544075012
    },
    {
      "source": "1472623",
      "target": "1222270",
      "weight": 0.769447922706604
    },
    {
      "source": "1042006",
      "target": "867270",
      "weight": 0.846489787101746
    },
    {
      "source": "1042006",
      "target": "1093400",
      "weight": 0.846134006977081
    },
    {
      "source": "1042006",
      "target": "2235578",
      "weight": 0.843155920505524
    },
    {
      "source": "1042006",
      "target": "618284",
      "weight": 0.832679688930512
    },
    {
      "source": "1042006",
      "target": "1536701",
      "weight": 0.82183575630188
    },
    {
      "source": "1355461",
      "target": "2364034",
      "weight": 0.84252792596817
    },
    {
      "source": "1355461",
      "target": "1824366",
      "weight": 0.836231291294098
    },
    {
      "source": "1355461",
      "target": "997049",
      "weight": 0.834306240081787
    },
    {
      "source": "1355461",
      "target": "1937177",
      "weight": 0.832731783390045
    },
    {
      "source": "1355461",
      "target": "2235578",
      "weight": 0.829745292663574
    },
    {
      "source": "296757",
      "target": "618284",
      "weight": 0.823048889636993
    },
    {
      "source": "296757",
      "target": "1554142",
      "weight": 0.81431394815445
    },
    {
      "source": "296757",
      "target": "1074170",
      "weight": 0.812834739685059
    },
    {
      "source": "296757",
      "target": "753816",
      "weight": 0.811841368675232
    },
    {
      "source": "296757",
      "target": "1682605",
      "weight": 0.81051367521286
    },
    {
      "source": "520648",
      "target": "1407666",
      "weight": 0.815182149410248
    },
    {
      "source": "520648",
      "target": "997049",
      "weight": 0.809833228588104
    },
    {
      "source": "520648",
      "target": "289403",
      "weight": 0.804668486118317
    },
    {
      "source": "520648",
      "target": "1156730",
      "weight": 0.803032517433167
    },
    {
      "source": "520648",
      "target": "2203888",
      "weight": 0.801791489124298
    },
    {
      "source": "546856",
      "target": "547066",
      "weight": 0.848618984222412
    },
    {
      "source": "546856",
      "target": "841809",
      "weight": 0.829024493694305
    },
    {
      "source": "546856",
      "target": "1962201",
      "weight": 0.825835824012756
    },
    {
      "source": "546856",
      "target": "1679011",
      "weight": 0.814112961292267
    },
    {
      "source": "546856",
      "target": "821966",
      "weight": 0.812822699546814
    },
    {
      "source": "911304",
      "target": "1251806",
      "weight": 0.866514205932617
    },
    {
      "source": "911304",
      "target": "1962201",
      "weight": 0.813217520713806
    },
    {
      "source": "911304",
      "target": "2235578",
      "weight": 0.812501788139343
    },
    {
      "source": "911304",
      "target": "2364034",
      "weight": 0.81182074546814
    },
    {
      "source": "911304",
      "target": "1099030",
      "weight": 0.810888171195984
    },
    {
      "source": "1469703",
      "target": "991294",
      "weight": 0.86420077085495
    },
    {
      "source": "1469703",
      "target": "1915663",
      "weight": 0.824461460113525
    },
    {
      "source": "1469703",
      "target": "2364034",
      "weight": 0.824262142181397
    },
    {
      "source": "1469703",
      "target": "1122662",
      "weight": 0.823457956314087
    },
    {
      "source": "1469703",
      "target": "2125619",
      "weight": 0.822835505008698
    },
    {
      "source": "1144358",
      "target": "1113546",
      "weight": 0.853137493133545
    },
    {
      "source": "1144358",
      "target": "2235578",
      "weight": 0.850449442863464
    },
    {
      "source": "1144358",
      "target": "1079871",
      "weight": 0.850299000740051
    },
    {
      "source": "1144358",
      "target": "485544",
      "weight": 0.847323179244995
    },
    {
      "source": "1144358",
      "target": "997049",
      "weight": 0.845480263233185
    },
    {
      "source": "1093400",
      "target": "618284",
      "weight": 0.909422159194946
    },
    {
      "source": "1093400",
      "target": "731505",
      "weight": 0.887870907783508
    },
    {
      "source": "1093400",
      "target": "1536701",
      "weight": 0.86840957403183
    },
    {
      "source": "1093400",
      "target": "2235578",
      "weight": 0.864403545856476
    },
    {
      "source": "1093400",
      "target": "887647",
      "weight": 0.863901972770691
    },
    {
      "source": "997049",
      "target": "1915663",
      "weight": 0.873544812202454
    },
    {
      "source": "997049",
      "target": "1554142",
      "weight": 0.863463282585144
    },
    {
      "source": "997049",
      "target": "1099030",
      "weight": 0.855910778045654
    },
    {
      "source": "997049",
      "target": "1176711",
      "weight": 0.854950189590454
    },
    {
      "source": "997049",
      "target": "1079871",
      "weight": 0.852864503860474
    },
    {
      "source": "991294",
      "target": "798699",
      "weight": 0.879592299461365
    },
    {
      "source": "991294",
      "target": "1469703",
      "weight": 0.86420077085495
    },
    {
      "source": "991294",
      "target": "1496556",
      "weight": 0.860556483268738
    },
    {
      "source": "991294",
      "target": "2364034",
      "weight": 0.860267281532288
    },
    {
      "source": "991294",
      "target": "1281930",
      "weight": 0.855518221855164
    },
    {
      "source": "1281930",
      "target": "2364034",
      "weight": 0.873745024204254
    },
    {
      "source": "1281930",
      "target": "1079871",
      "weight": 0.871910095214844
    },
    {
      "source": "1281930",
      "target": "1401067",
      "weight": 0.862263083457947
    },
    {
      "source": "1281930",
      "target": "962868",
      "weight": 0.855884432792664
    },
    {
      "source": "1281930",
      "target": "991294",
      "weight": 0.855518221855164
    },
    {
      "source": "885594",
      "target": "745155",
      "weight": 0.801442205905914
    },
    {
      "source": "885594",
      "target": "841809",
      "weight": 0.798984527587891
    },
    {
      "source": "885594",
      "target": "1377609",
      "weight": 0.7976313829422
    },
    {
      "source": "885594",
      "target": "1401067",
      "weight": 0.792839765548706
    },
    {
      "source": "885594",
      "target": "1679011",
      "weight": 0.790465116500855
    },
    {
      "source": "798699",
      "target": "887647",
      "weight": 0.883544921875
    },
    {
      "source": "798699",
      "target": "991294",
      "weight": 0.879592299461365
    },
    {
      "source": "798699",
      "target": "753816",
      "weight": 0.868856251239777
    },
    {
      "source": "798699",
      "target": "1537556",
      "weight": 0.856101155281067
    },
    {
      "source": "798699",
      "target": "1229577",
      "weight": 0.853745698928833
    },
    {
      "source": "2090730",
      "target": "2364034",
      "weight": 0.851280689239502
    },
    {
      "source": "2090730",
      "target": "1496556",
      "weight": 0.826339900493622
    },
    {
      "source": "2090730",
      "target": "1259109",
      "weight": 0.822890877723694
    },
    {
      "source": "2090730",
      "target": "2125619",
      "weight": 0.822377443313599
    },
    {
      "source": "2090730",
      "target": "1079871",
      "weight": 0.820912063121796
    },
    {
      "source": "1682605",
      "target": "1554142",
      "weight": 0.840502321720123
    },
    {
      "source": "1682605",
      "target": "1176711",
      "weight": 0.83162522315979
    },
    {
      "source": "1682605",
      "target": "1536701",
      "weight": 0.828500628471375
    },
    {
      "source": "1682605",
      "target": "2125619",
      "weight": 0.828158497810364
    },
    {
      "source": "1682605",
      "target": "2235578",
      "weight": 0.82810914516449
    },
    {
      "source": "1679011",
      "target": "841809",
      "weight": 0.901769697666168
    },
    {
      "source": "1679011",
      "target": "1079871",
      "weight": 0.857124984264374
    },
    {
      "source": "1679011",
      "target": "1122662",
      "weight": 0.855279803276062
    },
    {
      "source": "1679011",
      "target": "1401067",
      "weight": 0.852681457996368
    },
    {
      "source": "1679011",
      "target": "1377609",
      "weight": 0.849663138389587
    },
    {
      "source": "554476",
      "target": "731505",
      "weight": 0.860785841941834
    },
    {
      "source": "554476",
      "target": "618284",
      "weight": 0.852750599384308
    },
    {
      "source": "554476",
      "target": "1093400",
      "weight": 0.841125726699829
    },
    {
      "source": "554476",
      "target": "887647",
      "weight": 0.827930629253388
    },
    {
      "source": "554476",
      "target": "798699",
      "weight": 0.826998353004456
    },
    {
      "source": "2100974",
      "target": "1679011",
      "weight": 0.843673944473267
    },
    {
      "source": "2100974",
      "target": "2125619",
      "weight": 0.832933783531189
    },
    {
      "source": "2100974",
      "target": "841809",
      "weight": 0.829009294509888
    },
    {
      "source": "2100974",
      "target": "1915663",
      "weight": 0.822539925575256
    },
    {
      "source": "2100974",
      "target": "1554142",
      "weight": 0.819991648197174
    },
    {
      "source": "1122662",
      "target": "1679011",
      "weight": 0.855279803276062
    },
    {
      "source": "1122662",
      "target": "1079871",
      "weight": 0.851272761821747
    },
    {
      "source": "1122662",
      "target": "2364034",
      "weight": 0.849675476551056
    },
    {
      "source": "1122662",
      "target": "1496556",
      "weight": 0.844606995582581
    },
    {
      "source": "1122662",
      "target": "2125619",
      "weight": 0.842076063156128
    },
    {
      "source": "1176711",
      "target": "1079871",
      "weight": 0.864080131053925
    },
    {
      "source": "1176711",
      "target": "997049",
      "weight": 0.854950189590454
    },
    {
      "source": "1176711",
      "target": "2235578",
      "weight": 0.840130805969238
    },
    {
      "source": "1176711",
      "target": "1144358",
      "weight": 0.838834583759308
    },
    {
      "source": "1176711",
      "target": "1915663",
      "weight": 0.838529527187347
    },
    {
      "source": "1251806",
      "target": "911304",
      "weight": 0.866514205932617
    },
    {
      "source": "1251806",
      "target": "1113546",
      "weight": 0.855312466621399
    },
    {
      "source": "1251806",
      "target": "1079871",
      "weight": 0.854764580726624
    },
    {
      "source": "1251806",
      "target": "1496556",
      "weight": 0.850220263004303
    },
    {
      "source": "1251806",
      "target": "1679011",
      "weight": 0.845865607261658
    },
    {
      "source": "867270",
      "target": "1093400",
      "weight": 0.856653153896332
    },
    {
      "source": "867270",
      "target": "731505",
      "weight": 0.851700305938721
    },
    {
      "source": "867270",
      "target": "1042006",
      "weight": 0.846489787101746
    },
    {
      "source": "867270",
      "target": "618284",
      "weight": 0.84071010351181
    },
    {
      "source": "867270",
      "target": "1732740",
      "weight": 0.833596527576447
    },
    {
      "source": "1668640",
      "target": "2203888",
      "weight": 0.953672766685486
    },
    {
      "source": "1668640",
      "target": "2125619",
      "weight": 0.860029578208923
    },
    {
      "source": "1668640",
      "target": "1554142",
      "weight": 0.846652984619141
    },
    {
      "source": "1668640",
      "target": "1915663",
      "weight": 0.836981594562531
    },
    {
      "source": "1668640",
      "target": "1281930",
      "weight": 0.829140245914459
    },
    {
      "source": "821966",
      "target": "260559",
      "weight": 0.85975456237793
    },
    {
      "source": "821966",
      "target": "469826",
      "weight": 0.838652729988098
    },
    {
      "source": "821966",
      "target": "546856",
      "weight": 0.812822699546814
    },
    {
      "source": "821966",
      "target": "1962201",
      "weight": 0.810267806053162
    },
    {
      "source": "821966",
      "target": "289403",
      "weight": 0.809983789920807
    },
    {
      "source": "887647",
      "target": "1537556",
      "weight": 0.888173341751099
    },
    {
      "source": "887647",
      "target": "798699",
      "weight": 0.883544921875
    },
    {
      "source": "887647",
      "target": "1747974",
      "weight": 0.880366027355194
    },
    {
      "source": "887647",
      "target": "1381649",
      "weight": 0.876830041408539
    },
    {
      "source": "887647",
      "target": "753816",
      "weight": 0.869135677814484
    },
    {
      "source": "1332360",
      "target": "2235578",
      "weight": 0.842615067958832
    },
    {
      "source": "1332360",
      "target": "1519786",
      "weight": 0.830315589904785
    },
    {
      "source": "1332360",
      "target": "2364034",
      "weight": 0.829385280609131
    },
    {
      "source": "1332360",
      "target": "1259109",
      "weight": 0.81934779882431
    },
    {
      "source": "1332360",
      "target": "1962201",
      "weight": 0.81707626581192
    },
    {
      "source": "161582",
      "target": "1962201",
      "weight": 0.806878447532654
    },
    {
      "source": "161582",
      "target": "1320362",
      "weight": 0.79223108291626
    },
    {
      "source": "161582",
      "target": "469826",
      "weight": 0.790869235992432
    },
    {
      "source": "161582",
      "target": "1074170",
      "weight": 0.788343191146851
    },
    {
      "source": "161582",
      "target": "1332360",
      "weight": 0.788134455680847
    },
    {
      "source": "1824366",
      "target": "1281930",
      "weight": 0.844318330287933
    },
    {
      "source": "1824366",
      "target": "2235578",
      "weight": 0.842962026596069
    },
    {
      "source": "1824366",
      "target": "997049",
      "weight": 0.841654360294342
    },
    {
      "source": "1824366",
      "target": "2364034",
      "weight": 0.840630352497101
    },
    {
      "source": "1824366",
      "target": "2203888",
      "weight": 0.838154077529907
    },
    {
      "source": "753816",
      "target": "887647",
      "weight": 0.869135677814484
    },
    {
      "source": "753816",
      "target": "798699",
      "weight": 0.868856251239777
    },
    {
      "source": "753816",
      "target": "1381649",
      "weight": 0.859608888626099
    },
    {
      "source": "753816",
      "target": "1747974",
      "weight": 0.84842973947525
    },
    {
      "source": "753816",
      "target": "1537556",
      "weight": 0.845743834972382
    },
    {
      "source": "1461768",
      "target": "2364034",
      "weight": 0.799469232559204
    },
    {
      "source": "1461768",
      "target": "2235578",
      "weight": 0.787484526634216
    },
    {
      "source": "1461768",
      "target": "1092487",
      "weight": 0.775416910648346
    },
    {
      "source": "1461768",
      "target": "2109291",
      "weight": 0.770072042942047
    },
    {
      "source": "1461768",
      "target": "2203888",
      "weight": 0.769561469554901
    },
    {
      "source": "1801345",
      "target": "1802119",
      "weight": 0.861507058143616
    },
    {
      "source": "1801345",
      "target": "1536701",
      "weight": 0.842566549777985
    },
    {
      "source": "1801345",
      "target": "962868",
      "weight": 0.840137541294098
    },
    {
      "source": "1801345",
      "target": "1276394",
      "weight": 0.835394144058228
    },
    {
      "source": "1801345",
      "target": "2364034",
      "weight": 0.835058391094208
    },
    {
      "source": "1233461",
      "target": "1093400",
      "weight": 0.856344878673554
    },
    {
      "source": "1233461",
      "target": "618284",
      "weight": 0.853285551071167
    },
    {
      "source": "1233461",
      "target": "1537556",
      "weight": 0.849717020988464
    },
    {
      "source": "1233461",
      "target": "1732740",
      "weight": 0.848438262939453
    },
    {
      "source": "1233461",
      "target": "798699",
      "weight": 0.841456890106201
    },
    {
      "source": "731505",
      "target": "1093400",
      "weight": 0.887870907783508
    },
    {
      "source": "731505",
      "target": "618284",
      "weight": 0.884787499904633
    },
    {
      "source": "731505",
      "target": "554476",
      "weight": 0.860785841941834
    },
    {
      "source": "731505",
      "target": "1536701",
      "weight": 0.856016874313355
    },
    {
      "source": "731505",
      "target": "867270",
      "weight": 0.851700305938721
    },
    {
      "source": "1519786",
      "target": "1332360",
      "weight": 0.830315589904785
    },
    {
      "source": "1519786",
      "target": "1962201",
      "weight": 0.81688392162323
    },
    {
      "source": "1519786",
      "target": "547066",
      "weight": 0.805549502372742
    },
    {
      "source": "1519786",
      "target": "2235578",
      "weight": 0.800611913204193
    },
    {
      "source": "1519786",
      "target": "1937177",
      "weight": 0.793748497962952
    },
    {
      "source": "1413988",
      "target": "1079871",
      "weight": 0.824138283729553
    },
    {
      "source": "1413988",
      "target": "1801345",
      "weight": 0.819342613220215
    },
    {
      "source": "1413988",
      "target": "997049",
      "weight": 0.819311916828156
    },
    {
      "source": "1413988",
      "target": "1099030",
      "weight": 0.81820684671402
    },
    {
      "source": "1413988",
      "target": "1915663",
      "weight": 0.817290484905243
    },
    {
      "source": "962868",
      "target": "1536701",
      "weight": 0.864267706871033
    },
    {
      "source": "962868",
      "target": "1401067",
      "weight": 0.858993589878082
    },
    {
      "source": "962868",
      "target": "1281930",
      "weight": 0.855884432792664
    },
    {
      "source": "962868",
      "target": "1093400",
      "weight": 0.852039694786072
    },
    {
      "source": "962868",
      "target": "887647",
      "weight": 0.844747662544251
    },
    {
      "source": "1610287",
      "target": "1802119",
      "weight": 0.829521775245667
    },
    {
      "source": "1610287",
      "target": "1222270",
      "weight": 0.822695970535278
    },
    {
      "source": "1610287",
      "target": "131591",
      "weight": 0.804290473461151
    },
    {
      "source": "1610287",
      "target": "1381649",
      "weight": 0.801305055618286
    },
    {
      "source": "1610287",
      "target": "1801345",
      "weight": 0.796180486679077
    },
    {
      "source": "437384",
      "target": "131591",
      "weight": 0.805593848228455
    },
    {
      "source": "437384",
      "target": "2007304",
      "weight": 0.787128448486328
    },
    {
      "source": "437384",
      "target": "1377609",
      "weight": 0.779754996299744
    },
    {
      "source": "437384",
      "target": "1222270",
      "weight": 0.77765429019928
    },
    {
      "source": "437384",
      "target": "745155",
      "weight": 0.772020220756531
    },
    {
      "source": "1229577",
      "target": "798699",
      "weight": 0.853745698928833
    },
    {
      "source": "1229577",
      "target": "991294",
      "weight": 0.836190819740295
    },
    {
      "source": "1229577",
      "target": "887647",
      "weight": 0.829910635948181
    },
    {
      "source": "1229577",
      "target": "1093400",
      "weight": 0.826852381229401
    },
    {
      "source": "1229577",
      "target": "1537556",
      "weight": 0.82284539937973
    },
    {
      "source": "260559",
      "target": "821966",
      "weight": 0.85975456237793
    },
    {
      "source": "260559",
      "target": "289403",
      "weight": 0.843219101428986
    },
    {
      "source": "260559",
      "target": "469826",
      "weight": 0.841522395610809
    },
    {
      "source": "260559",
      "target": "1962201",
      "weight": 0.82222455739975
    },
    {
      "source": "260559",
      "target": "2125619",
      "weight": 0.820223927497864
    },
    {
      "source": "1259109",
      "target": "2364034",
      "weight": 0.847448468208313
    },
    {
      "source": "1259109",
      "target": "731505",
      "weight": 0.842317461967468
    },
    {
      "source": "1259109",
      "target": "991294",
      "weight": 0.830597639083862
    },
    {
      "source": "1259109",
      "target": "1355461",
      "weight": 0.823590576648712
    },
    {
      "source": "1259109",
      "target": "2090730",
      "weight": 0.822890877723694
    },
    {
      "source": "1173573",
      "target": "2125619",
      "weight": 0.841317534446716
    },
    {
      "source": "1173573",
      "target": "2364034",
      "weight": 0.839874386787415
    },
    {
      "source": "1173573",
      "target": "1554142",
      "weight": 0.835081100463867
    },
    {
      "source": "1173573",
      "target": "1937177",
      "weight": 0.832943201065064
    },
    {
      "source": "1173573",
      "target": "991294",
      "weight": 0.827497124671936
    },
    {
      "source": "1496556",
      "target": "2364034",
      "weight": 0.881965041160584
    },
    {
      "source": "1496556",
      "target": "1079871",
      "weight": 0.868310391902924
    },
    {
      "source": "1496556",
      "target": "2125619",
      "weight": 0.863284587860107
    },
    {
      "source": "1496556",
      "target": "991294",
      "weight": 0.860556483268738
    },
    {
      "source": "1496556",
      "target": "2235578",
      "weight": 0.858518183231354
    },
    {
      "source": "1222270",
      "target": "131591",
      "weight": 0.83450198173523
    },
    {
      "source": "1222270",
      "target": "2007304",
      "weight": 0.827034652233124
    },
    {
      "source": "1222270",
      "target": "1610287",
      "weight": 0.822695970535278
    },
    {
      "source": "1222270",
      "target": "821966",
      "weight": 0.806015253067017
    },
    {
      "source": "1222270",
      "target": "1407666",
      "weight": 0.79104483127594
    },
    {
      "source": "541656",
      "target": "1229101",
      "weight": 0.79662036895752
    },
    {
      "source": "541656",
      "target": "1083721",
      "weight": 0.783152461051941
    },
    {
      "source": "541656",
      "target": "745155",
      "weight": 0.782157778739929
    },
    {
      "source": "541656",
      "target": "546856",
      "weight": 0.765400528907776
    },
    {
      "source": "541656",
      "target": "1802119",
      "weight": 0.759039342403412
    },
    {
      "source": "1732740",
      "target": "1093400",
      "weight": 0.857069730758667
    },
    {
      "source": "1732740",
      "target": "618284",
      "weight": 0.855054259300232
    },
    {
      "source": "1732740",
      "target": "887647",
      "weight": 0.851320624351502
    },
    {
      "source": "1732740",
      "target": "1233461",
      "weight": 0.848438262939453
    },
    {
      "source": "1732740",
      "target": "1747974",
      "weight": 0.84403121471405
    },
    {
      "source": "618284",
      "target": "1093400",
      "weight": 0.909422159194946
    },
    {
      "source": "618284",
      "target": "731505",
      "weight": 0.884787499904633
    },
    {
      "source": "618284",
      "target": "2235578",
      "weight": 0.875064969062805
    },
    {
      "source": "618284",
      "target": "1537556",
      "weight": 0.866984486579895
    },
    {
      "source": "618284",
      "target": "1747974",
      "weight": 0.862497389316559
    },
    {
      "source": "669837",
      "target": "997049",
      "weight": 0.821537137031555
    },
    {
      "source": "669837",
      "target": "1401067",
      "weight": 0.816473007202148
    },
    {
      "source": "669837",
      "target": "1083721",
      "weight": 0.812942087650299
    },
    {
      "source": "669837",
      "target": "1176711",
      "weight": 0.810112416744232
    },
    {
      "source": "669837",
      "target": "1332360",
      "weight": 0.808529436588287
    },
    {
      "source": "1099030",
      "target": "997049",
      "weight": 0.855910778045654
    },
    {
      "source": "1099030",
      "target": "1554142",
      "weight": 0.819364786148071
    },
    {
      "source": "1099030",
      "target": "1413988",
      "weight": 0.81820684671402
    },
    {
      "source": "1099030",
      "target": "1144358",
      "weight": 0.817998647689819
    },
    {
      "source": "1099030",
      "target": "2235578",
      "weight": 0.815483331680298
    },
    {
      "source": "1156730",
      "target": "2364034",
      "weight": 0.86937689781189
    },
    {
      "source": "1156730",
      "target": "485544",
      "weight": 0.854965746402741
    },
    {
      "source": "1156730",
      "target": "1281930",
      "weight": 0.84760856628418
    },
    {
      "source": "1156730",
      "target": "2235578",
      "weight": 0.839209079742432
    },
    {
      "source": "1156730",
      "target": "1079871",
      "weight": 0.833857774734497
    },
    {
      "source": "229453",
      "target": "1962201",
      "weight": 0.809955656528473
    },
    {
      "source": "229453",
      "target": "546856",
      "weight": 0.791947245597839
    },
    {
      "source": "229453",
      "target": "2235578",
      "weight": 0.787808537483215
    },
    {
      "source": "229453",
      "target": "2125619",
      "weight": 0.786833703517914
    },
    {
      "source": "229453",
      "target": "1229101",
      "weight": 0.784226775169373
    },
    {
      "source": "1092487",
      "target": "1554142",
      "weight": 0.809304654598236
    },
    {
      "source": "1092487",
      "target": "1407666",
      "weight": 0.804377257823944
    },
    {
      "source": "1092487",
      "target": "289403",
      "weight": 0.802763044834137
    },
    {
      "source": "1092487",
      "target": "1074170",
      "weight": 0.801886022090912
    },
    {
      "source": "1092487",
      "target": "2007304",
      "weight": 0.788973212242127
    },
    {
      "source": "1025928",
      "target": "2235578",
      "weight": 0.844319820404053
    },
    {
      "source": "1025928",
      "target": "1536701",
      "weight": 0.828064799308777
    },
    {
      "source": "1025928",
      "target": "1377609",
      "weight": 0.825649797916412
    },
    {
      "source": "1025928",
      "target": "1915663",
      "weight": 0.823545098304749
    },
    {
      "source": "1025928",
      "target": "841809",
      "weight": 0.821395397186279
    },
    {
      "source": "2068082",
      "target": "1413988",
      "weight": 0.781616389751434
    },
    {
      "source": "2068082",
      "target": "2235578",
      "weight": 0.77672278881073
    },
    {
      "source": "2068082",
      "target": "1377609",
      "weight": 0.774592638015747
    },
    {
      "source": "2068082",
      "target": "1554142",
      "weight": 0.769254863262177
    },
    {
      "source": "2068082",
      "target": "1320362",
      "weight": 0.767883062362671
    },
    {
      "source": "2327865",
      "target": "960478",
      "weight": 0.880781710147858
    },
    {
      "source": "2327865",
      "target": "2235578",
      "weight": 0.824451565742493
    },
    {
      "source": "2327865",
      "target": "2364034",
      "weight": 0.813771963119507
    },
    {
      "source": "2327865",
      "target": "1079871",
      "weight": 0.806927919387817
    },
    {
      "source": "2327865",
      "target": "1099030",
      "weight": 0.802976131439209
    },
    {
      "source": "469826",
      "target": "997049",
      "weight": 0.846194624900818
    },
    {
      "source": "469826",
      "target": "260559",
      "weight": 0.841522395610809
    },
    {
      "source": "469826",
      "target": "821966",
      "weight": 0.838652729988098
    },
    {
      "source": "469826",
      "target": "485544",
      "weight": 0.834000289440155
    },
    {
      "source": "469826",
      "target": "1156730",
      "weight": 0.815125703811646
    },
    {
      "source": "1276394",
      "target": "1801345",
      "weight": 0.835394144058228
    },
    {
      "source": "1276394",
      "target": "1802119",
      "weight": 0.834778964519501
    },
    {
      "source": "1276394",
      "target": "1079871",
      "weight": 0.818124294281006
    },
    {
      "source": "1276394",
      "target": "1413988",
      "weight": 0.810306012630463
    },
    {
      "source": "1276394",
      "target": "2235578",
      "weight": 0.799608528614044
    },
    {
      "source": "693186",
      "target": "997049",
      "weight": 0.839171707630158
    },
    {
      "source": "693186",
      "target": "991294",
      "weight": 0.837041854858398
    },
    {
      "source": "693186",
      "target": "1079871",
      "weight": 0.829090416431427
    },
    {
      "source": "693186",
      "target": "2109291",
      "weight": 0.824298202991486
    },
    {
      "source": "693186",
      "target": "1496556",
      "weight": 0.820866465568543
    },
    {
      "source": "2060759",
      "target": "1915663",
      "weight": 0.807753682136536
    },
    {
      "source": "2060759",
      "target": "731505",
      "weight": 0.800845503807068
    },
    {
      "source": "2060759",
      "target": "1554142",
      "weight": 0.799166917800903
    },
    {
      "source": "2060759",
      "target": "1332360",
      "weight": 0.799088180065155
    },
    {
      "source": "2060759",
      "target": "862266",
      "weight": 0.798357725143433
    },
    {
      "source": "1552605",
      "target": "2125619",
      "weight": 0.826495051383972
    },
    {
      "source": "1552605",
      "target": "2364034",
      "weight": 0.809487879276276
    },
    {
      "source": "1552605",
      "target": "1281930",
      "weight": 0.808311939239502
    },
    {
      "source": "1552605",
      "target": "1401067",
      "weight": 0.804568648338318
    },
    {
      "source": "1552605",
      "target": "962868",
      "weight": 0.800570726394653
    }
  ]
}